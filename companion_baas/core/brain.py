#!/usr/bin/env python3
"""
Companion Brain - The Core AI Engine
=====================================

This is the heart of Companion BaaS. It handles:
- Model selection and routing
- Context management
- Response generation
- Caching and optimization
- Error handling and fallbacks

This brain can power ANY type of application!
"""

import sys
import os
import logging
from typing import Dict, List, Optional, Any, Union, Callable, Tuple
from datetime import datetime, timedelta
import uuid
import time
from enum import Enum
import asyncio
import hashlib
import json

# Optional numpy import for environments that don't have it (like Vercel)
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    np = None
    HAS_NUMPY = False

# Add parent directories to path
current_dir = os.path.dirname(os.path.abspath(__file__))
baas_dir = os.path.dirname(current_dir)  # companion_baas directory
parent_dir = os.path.dirname(baas_dir)   # Companion deepthink directory
website_dir = os.path.join(parent_dir, 'website')

# Add companion_baas to path first for phase imports
sys.path.insert(0, baas_dir)

logger = logging.getLogger(__name__)

# ============================================================================
# CIRCUIT BREAKER PATTERN (Tier 2 Reliability)
# ============================================================================

# ============================================================================
# SEMANTIC CACHE (Tier 3 - Advanced Caching)
# ============================================================================

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    logger.debug("sentence-transformers not available for semantic caching")

class SemanticCache:
    """
    Semantic cache using embeddings for similarity-based matching.
    Achieves 70-80% hit rate vs 20-30% for exact match caching.
    """
    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', similarity_threshold: float = 0.85, max_size: int = 1000):
        """
        Initialize semantic cache
        
        Args:
            model_name: Sentence transformer model name
            similarity_threshold: Minimum cosine similarity for cache hit (0.0-1.0)
            max_size: Maximum cache entries
        """
        self.similarity_threshold = similarity_threshold
        self.max_size = max_size
        self.cache: List[Dict[str, Any]] = []
        self.model = None
        self.hits = 0
        self.misses = 0
        self.model_name = model_name  # Store model name for lazy loading
        
        # Don't initialize model during startup - lazy load when needed
        logger.info("âœ… Semantic cache initialized (lazy loading)")
    
    def _ensure_model_loaded(self):
        """Lazy load the sentence transformer model when needed"""
        if self.model is None and SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self.model = SentenceTransformer(self.model_name)
                logger.info(f"âœ… Semantic cache model loaded: {self.model_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load semantic cache model: {e}")
                self.model = None
    
    def _compute_embedding(self, text: str) -> Optional[list]:
        """Compute embedding for text"""
        if not HAS_NUMPY:
            return None  # Return None when numpy is not available
        self._ensure_model_loaded()
        if not self.model:
            return None
        try:
            return self.model.encode(text, convert_to_numpy=True)
        except Exception as e:
            logger.warning(f"Embedding computation failed: {e}")
            return None
    
    def _cosine_similarity(self, vec1, vec2) -> float:
        """Compute cosine similarity between two vectors"""
        if not HAS_NUMPY:
            return 0.0  # Return default similarity when numpy is not available
        try:
            return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))
        except Exception:
            return 0.0
    
    def get(self, query: str, context: Optional[Dict] = None) -> Optional[str]:
        """
        Retrieve cached response for similar query
        
        Args:
            query: Input query
            context: Optional context for matching
            
        Returns:
            Cached response if similar query found, else None
        """
        if not self.model or not self.cache:
            self.misses += 1
            return None
        
        # Compute query embedding
        query_embedding = self._compute_embedding(query)
        if query_embedding is None:
            self.misses += 1
            return None
        
        # Find most similar cached query
        best_match = None
        best_similarity = 0.0
        
        for entry in self.cache:
            similarity = self._cosine_similarity(query_embedding, entry['embedding'])
            
            # Check context match if provided
            context_match = True
            if context and entry.get('context'):
                context_match = (context.get('app_type') == entry['context'].get('app_type'))
            
            if similarity > best_similarity and similarity >= self.similarity_threshold and context_match:
                best_similarity = similarity
                best_match = entry
        
        if best_match:
            self.hits += 1
            logger.info(f"âœ… Semantic cache HIT (similarity: {best_similarity:.3f})")
            return best_match['response']
        
        self.misses += 1
        return None
    
    def set(self, query: str, response: str, context: Optional[Dict] = None):
        """
        Cache query-response pair with embedding
        
        Args:
            query: Input query
            response: Generated response
            context: Optional context
        """
        if not self.model:
            return
        
        # Compute embedding
        embedding = self._compute_embedding(query)
        if embedding is None:
            return
        
        # Create cache entry
        entry = {
            'query': query,
            'response': response,
            'embedding': embedding,
            'context': context,
            'timestamp': datetime.now().isoformat()
        }
        
        # Add to cache
        self.cache.append(entry)
        
        # Enforce max size (remove oldest)
        if len(self.cache) > self.max_size:
            self.cache.pop(0)
        
        logger.debug(f"ðŸ“¦ Cached semantic entry (total: {len(self.cache)})")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total = self.hits + self.misses
        hit_rate = (self.hits / total * 100) if total > 0 else 0.0
        
        return {
            'enabled': self.model is not None,
            'hits': self.hits,
            'misses': self.misses,
            'total_requests': total,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache),
            'max_size': self.max_size,
            'threshold': self.similarity_threshold
        }
    
    def clear(self):
        """Clear cache"""
        self.cache.clear()
        logger.info("ðŸ§¹ Semantic cache cleared")

class CircuitState(Enum):
    """Circuit breaker states"""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failures detected, blocking requests
    HALF_OPEN = "half_open"  # Testing if service recovered

# ============================================================================
# MULTI-MODEL CONSENSUS (Tier 3 - Enhanced Accuracy)
# ============================================================================

class MultiModelConsensus:
    """
    Query multiple models and combine results for higher accuracy.
    Useful for critical queries that need validation.
    """
    def __init__(self, brain_instance):
        self.brain = brain_instance
        self.consensus_queries = 0
        self.total_models_queried = 0
        
    async def query_with_consensus(
        self,
        message: str,
        models: Optional[List[str]] = None,
        min_agreement: float = 0.6
    ) -> Dict[str, Any]:
        """
        Query multiple models and combine responses
        
        Args:
            message: Query message
            models: List of model names (default: 3 recommended models)
            min_agreement: Minimum agreement threshold (0.0-1.0)
            
        Returns:
            Dict with combined response, confidence, and individual results
        """
        if not models:
            # Use diverse model set for consensus (using OpenRouter for reliability)
            models = ['qwen-4b', 'qwen-4b', 'deepseek-coder']  # Removed phi-2-reasoner
        
        self.consensus_queries += 1
        logger.info(f"ðŸ¤ Running consensus query with {len(models)} models")
        
        # Query all models in parallel
        tasks = []
        for model in models:
            tasks.append(self._query_model(message, model))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        self.total_models_queried += len(models)
        
        # Filter successful results
        successful_results = []
        for i, result in enumerate(results):
            if not isinstance(result, Exception) and result.get('success'):
                successful_results.append({
                    'model': models[i],
                    'response': result.get('response', ''),
                    'confidence': result.get('confidence', 0.5)
                })
        
        if not successful_results:
            return {
                'success': False,
                'error': 'All models failed',
                'response': None
            }
        
        # Combine responses
        combined = self._combine_responses(successful_results, min_agreement)
        
        return {
            'success': True,
            'response': combined['response'],
            'confidence': combined['confidence'],
            'models_used': len(successful_results),
            'individual_results': successful_results,
            'metadata': {
                'consensus_type': combined['method'],
                'agreement_score': combined['agreement']
            }
        }
    
    async def _query_model(self, message: str, model: str) -> Dict[str, Any]:
        """Query a single model"""
        try:
            # Use brain's LLM wrapper
            loop = asyncio.get_event_loop()
            
            def _call():
                if 'bytez' in self.brain.providers:
                    bytez = self.brain.providers['bytez']
                    return bytez.generate(
                        messages=[{'role': 'user', 'content': message}],
                        model=model
                    )
                return {'success': False, 'error': 'No provider available'}
            
            result = await loop.run_in_executor(None, _call)
            return result
        except Exception as e:
            logger.warning(f"Model {model} query failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _combine_responses(self, results: List[Dict], min_agreement: float) -> Dict[str, Any]:
        """
        Combine multiple model responses
        
        Strategies:
        1. If responses very similar â†’ use highest confidence
        2. If responses differ â†’ combine or flag disagreement
        3. Weight by model confidence scores
        """
        if len(results) == 1:
            return {
                'response': results[0]['response'],
                'confidence': results[0]['confidence'],
                'method': 'single',
                'agreement': 1.0
            }
        
        # Calculate response similarity (simple word overlap)
        similarities = []
        for i in range(len(results)):
            for j in range(i+1, len(results)):
                sim = self._response_similarity(results[i]['response'], results[j]['response'])
                similarities.append(sim)
        
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
        
        # High agreement â†’ use best response
        if avg_similarity >= min_agreement:
            best = max(results, key=lambda x: x['confidence'])
            return {
                'response': best['response'],
                'confidence': best['confidence'] * avg_similarity,
                'method': 'best_with_consensus',
                'agreement': avg_similarity
            }
        
        # Low agreement â†’ combine responses
        combined_text = self._merge_responses([r['response'] for r in results])
        avg_confidence = sum(r['confidence'] for r in results) / len(results)
        
        return {
            'response': combined_text,
            'confidence': avg_confidence * 0.8,  # Penalty for disagreement
            'method': 'combined_diverse',
            'agreement': avg_similarity
        }
    
    def _response_similarity(self, resp1: str, resp2: str) -> float:
        """Calculate similarity between two responses (simple word overlap)"""
        try:
            words1 = set(resp1.lower().split())
            words2 = set(resp2.lower().split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1 & words2)
            union = len(words1 | words2)
            
            return intersection / union if union > 0 else 0.0
        except Exception:
            return 0.0
    
    def _merge_responses(self, responses: List[str]) -> str:
        """Merge multiple responses into one"""
        # Simple merge: combine unique insights
        merged = f"Multiple models consulted:\n\n"
        for i, resp in enumerate(responses, 1):
            merged += f"Perspective {i}: {resp[:200]}...\n\n"
        
        merged += "Summary: Models show some disagreement. Consider multiple perspectives."
        return merged
    
    def get_stats(self) -> Dict[str, Any]:
        """Get consensus statistics"""
        avg_models = self.total_models_queried / self.consensus_queries if self.consensus_queries > 0 else 0
        
        return {
            'consensus_queries': self.consensus_queries,
            'total_models_queried': self.total_models_queried,
            'avg_models_per_query': avg_models
        }


class PromptOptimizer:
    """
    A/B testing framework for prompt optimization.
    Tracks performance of different prompt variations and automatically selects best performers.
    """
    
    def __init__(self, max_variants: int = 5, min_samples_per_variant: int = 10):
        """
        Initialize prompt optimizer
        
        Args:
            max_variants: Maximum number of prompt variants to test simultaneously
            min_samples_per_variant: Minimum samples needed before making optimization decisions
        """
        self.max_variants = max_variants
        self.min_samples_per_variant = min_samples_per_variant
        
        # Structure: {task_type: {variant_id: {prompt: str, stats: {...}}}}
        self.variants: Dict[str, Dict[str, Dict[str, Any]]] = {}
        
        # Track which variant is currently best for each task
        self.best_variants: Dict[str, str] = {}
        
        # Total experiments run
        self.total_experiments = 0
        
        logger.info("âœ… Prompt optimizer initialized")
    
    def register_variant(self, task_type: str, variant_id: str, prompt_template: str):
        """
        Register a new prompt variant for testing
        
        Args:
            task_type: Type of task (e.g., 'code_generation', 'explanation', 'summarization')
            variant_id: Unique identifier for this variant
            prompt_template: The prompt template to test
        """
        if task_type not in self.variants:
            self.variants[task_type] = {}
        
        if len(self.variants[task_type]) >= self.max_variants:
            logger.warning(f"âš ï¸ Max variants reached for {task_type}, not registering {variant_id}")
            return
        
        self.variants[task_type][variant_id] = {
            'prompt': prompt_template,
            'stats': {
                'uses': 0,
                'successes': 0,
                'failures': 0,
                'total_latency': 0.0,
                'avg_latency': 0.0,
                'success_rate': 0.0,
                'avg_response_length': 0,
                'total_response_length': 0
            }
        }
        
        logger.info(f"ðŸ“ Registered prompt variant: {task_type}/{variant_id}")
    
    def select_variant(self, task_type: str, exploration_rate: float = 0.2) -> Optional[str]:
        """
        Select which prompt variant to use (epsilon-greedy strategy)
        
        Args:
            task_type: Type of task
            exploration_rate: Probability of exploring non-best variants (0.0-1.0)
            
        Returns:
            variant_id to use, or None if no variants registered
        """
        if task_type not in self.variants or not self.variants[task_type]:
            return None
        
        import random
        
        # Exploration: randomly select a variant
        if random.random() < exploration_rate:
            return random.choice(list(self.variants[task_type].keys()))
        
        # Exploitation: use best known variant if determined
        if task_type in self.best_variants:
            best_id = self.best_variants[task_type]
            if best_id in self.variants[task_type]:
                return best_id
        
        # If no best variant yet, select least-tested variant
        variants = self.variants[task_type]
        return min(variants.keys(), key=lambda v: variants[v]['stats']['uses'])
    
    def record_result(
        self, 
        task_type: str, 
        variant_id: str, 
        success: bool, 
        latency: float,
        response_length: int = 0
    ):
        """
        Record the result of using a prompt variant
        
        Args:
            task_type: Type of task
            variant_id: Variant that was used
            success: Whether the prompt succeeded
            latency: Response latency in seconds
            response_length: Length of response (characters)
        """
        if task_type not in self.variants or variant_id not in self.variants[task_type]:
            logger.warning(f"âš ï¸ Unknown variant: {task_type}/{variant_id}")
            return
        
        stats = self.variants[task_type][variant_id]['stats']
        stats['uses'] += 1
        
        if success:
            stats['successes'] += 1
        else:
            stats['failures'] += 1
        
        stats['total_latency'] += latency
        stats['avg_latency'] = stats['total_latency'] / stats['uses']
        stats['success_rate'] = stats['successes'] / stats['uses'] * 100
        
        stats['total_response_length'] += response_length
        stats['avg_response_length'] = stats['total_response_length'] / stats['uses']
        
        self.total_experiments += 1
        
        # Update best variant if we have enough samples
        self._update_best_variant(task_type)
    
    def _update_best_variant(self, task_type: str):
        """
        Update which variant is best for a task based on collected stats
        
        Uses a composite score: success_rate * 0.7 + (1 / normalized_latency) * 0.3
        """
        if task_type not in self.variants:
            return
        
        variants = self.variants[task_type]
        
        # Only update if all variants have minimum samples
        if not all(v['stats']['uses'] >= self.min_samples_per_variant for v in variants.values()):
            return
        
        # Calculate composite score for each variant
        scores = {}
        max_latency = max(v['stats']['avg_latency'] for v in variants.values() if v['stats']['avg_latency'] > 0) or 1.0
        
        for vid, variant in variants.items():
            stats = variant['stats']
            
            # Normalize success rate (0-100 -> 0-1)
            success_score = stats['success_rate'] / 100
            
            # Normalize latency (lower is better, 0-1 scale)
            latency_score = 1.0 - (stats['avg_latency'] / max_latency) if max_latency > 0 else 1.0
            
            # Composite score (70% success rate, 30% speed)
            scores[vid] = success_score * 0.7 + latency_score * 0.3
        
        # Select best variant
        if scores:
            best_id = max(scores.keys(), key=lambda k: scores[k])
            
            if task_type not in self.best_variants or self.best_variants[task_type] != best_id:
                old_best = self.best_variants.get(task_type, 'none')
                self.best_variants[task_type] = best_id
                logger.info(f"ðŸ† Best variant for {task_type}: {best_id} (score: {scores[best_id]:.3f}, was: {old_best})")
    
    def get_variant_prompt(self, task_type: str, variant_id: str) -> Optional[str]:
        """Get the prompt template for a specific variant"""
        if task_type in self.variants and variant_id in self.variants[task_type]:
            return self.variants[task_type][variant_id]['prompt']
        return None
    
    def get_stats(self, task_type: Optional[str] = None) -> Dict[str, Any]:
        """
        Get optimization statistics
        
        Args:
            task_type: Optional task type to filter stats, or None for all tasks
            
        Returns:
            Statistics about prompt optimization experiments
        """
        if task_type:
            if task_type not in self.variants:
                return {'error': f'Unknown task type: {task_type}'}
            
            return {
                'task_type': task_type,
                'variants': {
                    vid: {
                        'uses': v['stats']['uses'],
                        'success_rate': v['stats']['success_rate'],
                        'avg_latency': v['stats']['avg_latency']
                    }
                    for vid, v in self.variants[task_type].items()
                },
                'best_variant': self.best_variants.get(task_type, 'undetermined')
            }
        else:
            # Global stats
            return {
                'total_experiments': self.total_experiments,
                'task_types': list(self.variants.keys()),
                'total_variants': sum(len(v) for v in self.variants.values()),
                'best_variants': self.best_variants
            }


class PerformanceMonitor:
    """
    Advanced observability with percentile calculations (P50, P95, P99).
    Tracks latency distributions and provides detailed performance insights.
    """
    
    def __init__(self, max_samples: int = 10000):
        """
        Initialize performance monitor
        
        Args:
            max_samples: Maximum number of samples to keep per metric (rolling window)
        """
        self.max_samples = max_samples
        
        # Store latency samples for different operations
        # Structure: {metric_name: [latency1, latency2, ...]}
        self.metrics: Dict[str, List[float]] = {}
        
        # Counters for operations
        self.operation_counts: Dict[str, int] = {}
        
        logger.info("âœ… Performance monitor initialized")
    
    def record(self, metric_name: str, latency: float):
        """
        Record a latency measurement
        
        Args:
            metric_name: Name of the metric (e.g., 'think', 'llm_call', 'cache_lookup')
            latency: Latency in seconds
        """
        if metric_name not in self.metrics:
            self.metrics[metric_name] = []
            self.operation_counts[metric_name] = 0
        
        self.metrics[metric_name].append(latency)
        self.operation_counts[metric_name] += 1
        
        # Enforce rolling window
        if len(self.metrics[metric_name]) > self.max_samples:
            self.metrics[metric_name].pop(0)
    
    def calculate_percentile(self, values: List[float], percentile: float) -> float:
        """
        Calculate percentile value from a list of numbers
        
        Args:
            values: List of numeric values
            percentile: Percentile to calculate (0-100)
            
        Returns:
            Percentile value
        """
        if not values:
            return 0.0
        
        sorted_values = sorted(values)
        k = (len(sorted_values) - 1) * (percentile / 100.0)
        f = int(k)
        c = f + 1
        
        if c >= len(sorted_values):
            return sorted_values[-1]
        
        # Linear interpolation between two closest ranks
        return sorted_values[f] + (k - f) * (sorted_values[c] - sorted_values[f])
    
    def get_percentiles(self, metric_name: str) -> Dict[str, float]:
        """
        Get P50, P95, P99 percentiles for a metric
        
        Args:
            metric_name: Name of the metric
            
        Returns:
            Dict with p50, p95, p99, min, max, mean, count
        """
        if metric_name not in self.metrics or not self.metrics[metric_name]:
            return {
                'p50': 0.0,
                'p95': 0.0,
                'p99': 0.0,
                'min': 0.0,
                'max': 0.0,
                'mean': 0.0,
                'count': 0
            }
        
        values = self.metrics[metric_name]
        
        return {
            'p50': self.calculate_percentile(values, 50),
            'p95': self.calculate_percentile(values, 95),
            'p99': self.calculate_percentile(values, 99),
            'min': min(values),
            'max': max(values),
            'mean': sum(values) / len(values),
            'count': len(values)
        }
    
    def get_histogram(self, metric_name: str, num_buckets: int = 10) -> Dict[str, Any]:
        """
        Get latency histogram for a metric
        
        Args:
            metric_name: Name of the metric
            num_buckets: Number of histogram buckets
            
        Returns:
            Dict with bucket ranges and counts
        """
        if metric_name not in self.metrics or not self.metrics[metric_name]:
            return {
                'buckets': [],
                'counts': [],
                'total': 0
            }
        
        values = self.metrics[metric_name]
        min_val = min(values)
        max_val = max(values)
        
        if min_val == max_val:
            # All values are the same
            return {
                'buckets': [f"{min_val:.3f}"],
                'counts': [len(values)],
                'total': len(values)
            }
        
        # Create buckets
        bucket_size = (max_val - min_val) / num_buckets
        buckets = []
        counts = [0] * num_buckets
        
        for i in range(num_buckets):
            bucket_start = min_val + i * bucket_size
            bucket_end = bucket_start + bucket_size
            buckets.append(f"{bucket_start:.3f}-{bucket_end:.3f}")
        
        # Count values in each bucket
        for value in values:
            bucket_idx = min(int((value - min_val) / bucket_size), num_buckets - 1)
            counts[bucket_idx] += 1
        
        return {
            'buckets': buckets,
            'counts': counts,
            'total': len(values)
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get comprehensive performance statistics
        
        Returns:
            Dict with percentiles and histograms for all tracked metrics
        """
        stats = {
            'total_metrics': len(self.metrics),
            'total_samples': sum(len(v) for v in self.metrics.values()),
            'metrics': {}
        }
        
        for metric_name in self.metrics.keys():
            stats['metrics'][metric_name] = {
                'percentiles': self.get_percentiles(metric_name),
                'total_operations': self.operation_counts.get(metric_name, 0)
            }
        
        return stats
    
    def reset(self, metric_name: Optional[str] = None):
        """
        Reset metrics
        
        Args:
            metric_name: Optional metric to reset, or None to reset all
        """
        if metric_name:
            if metric_name in self.metrics:
                self.metrics[metric_name] = []
                self.operation_counts[metric_name] = 0
        else:
            self.metrics = {}
            self.operation_counts = {}


class CircuitBreaker:
    """
    Circuit breaker to prevent cascading failures.
    Monitors component health and disables failing components temporarily.
    """
    def __init__(self, failure_threshold: int = 3, recovery_timeout: int = 60, name: str = "unknown"):
        self.name = name
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = CircuitState.CLOSED
        
    def call(self, func: Callable, *args, **kwargs):
        """Execute function with circuit breaker protection"""
        if self.state == CircuitState.OPEN:
            # Check if recovery timeout has passed
            if self.last_failure_time and \
               (datetime.now() - self.last_failure_time).total_seconds() >= self.recovery_timeout:
                logger.info(f"ðŸ”„ Circuit breaker '{self.name}' entering HALF_OPEN state for recovery test")
                self.state = CircuitState.HALF_OPEN
            else:
                raise RuntimeError(f"Circuit breaker '{self.name}' is OPEN, blocking request")
        
        try:
            result = func(*args, **kwargs)
            # Success - reset failure count
            if self.state == CircuitState.HALF_OPEN:
                logger.info(f"âœ… Circuit breaker '{self.name}' recovered, closing circuit")
                self.state = CircuitState.CLOSED
            self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            self.last_failure_time = datetime.now()
            
            if self.failure_count >= self.failure_threshold:
                logger.error(f"ðŸ”´ Circuit breaker '{self.name}' OPENED after {self.failure_count} failures")
                self.state = CircuitState.OPEN
            else:
                logger.warning(f"âš ï¸ Circuit breaker '{self.name}' failure {self.failure_count}/{self.failure_threshold}")
            
            raise e
    
    def reset(self):
        """Manually reset circuit breaker"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
        logger.info(f"ðŸ”„ Circuit breaker '{self.name}' manually reset")
    
    def get_state(self) -> Dict[str, Any]:
        """Get current circuit breaker state"""
        return {
            'name': self.name,
            'state': self.state.value,
            'failure_count': self.failure_count,
            'last_failure': self.last_failure_time.isoformat() if self.last_failure_time else None
        }

# ============================================================================
# PHASE 1: KNOWLEDGE LAYER
# ============================================================================
try:
    from companion_baas.config import get_config  # Import config from companion_baas/config
    from companion_baas.knowledge.elasticsearch_client import ElasticsearchClient
    from companion_baas.knowledge.vector_store import VectorStore
    from companion_baas.knowledge.retriever import KnowledgeRetriever  # Fixed: retriever not knowledge_retriever
    PHASE1_AVAILABLE = True
    logger.info("âœ… Phase 1 (Knowledge Layer) loaded")
except ImportError as e:
    PHASE1_AVAILABLE = False
    logger.warning(f"âš ï¸  Phase 1 not available: {e}")

# ============================================================================
# PHASE 2: SEARCH LAYER
# ============================================================================
try:
    from companion_baas.search.meilisearch_client import MeilisearchClient
    from companion_baas.search.search_engine import SearchEngine
    PHASE2_AVAILABLE = True
    logger.info("âœ… Phase 2 (Search Layer) loaded")
except ImportError as e:
    PHASE2_AVAILABLE = False
    logger.warning(f"âš ï¸  Phase 2 not available: {e}")

# ============================================================================
# PHASE 3: WEB INTELLIGENCE
# ============================================================================
try:
    from companion_baas.web_intelligence.crawler import WebContentCrawler  # Fixed: WebContentCrawler not WebCrawler
    from companion_baas.web_intelligence.api_clients.news_api import NewsAPIClient
    from companion_baas.web_intelligence.api_clients.search_api import WebSearchClient  # Fixed: WebSearchClient not SearchAPIClient
    PHASE3_AVAILABLE = True
    logger.info("âœ… Phase 3 (Web Intelligence) loaded")
except ImportError as e:
    PHASE3_AVAILABLE = False
    logger.warning(f"âš ï¸  Phase 3 not available: {e}")

# ============================================================================
# PHASE 4: EXECUTION & GENERATION
# ============================================================================
try:
    from companion_baas.execution.code_executor import CodeExecutor
    from companion_baas.tools.tool_registry import ToolRegistry
    from companion_baas.tools.tool_executor import ToolExecutor
    from companion_baas.tools.builtin_tools import register_builtin_tools
    PHASE4_AVAILABLE = True
    logger.info("âœ… Phase 4 (Execution & Generation) loaded")
except ImportError as e:
    PHASE4_AVAILABLE = False
    logger.warning(f"âš ï¸  Phase 4 not available: {e}")

# ============================================================================
# PHASE 5: OPTIMIZATION
# ============================================================================
try:
    from companion_baas.optimization.profiler import profiler
    from companion_baas.optimization.cache_optimizer import cache_optimizer
    from companion_baas.optimization.monitoring import monitor
    PHASE5_AVAILABLE = True
    logger.info("âœ… Phase 5 (Optimization) loaded")
except ImportError as e:
    PHASE5_AVAILABLE = False
    logger.warning(f"âš ï¸  Phase 5 not available: {e}")

# ============================================================================
# LEGACY COMPONENTS (for backward compatibility)
# ============================================================================
# Temporarily add website to path for legacy imports
api_wrapper = None
generate_companion_response = None
APIResponse = None
search_wrapper = None
SearchResult = None
response_cache = None

sys.path.insert(0, website_dir)
try:
    from api_wrapper import api_wrapper, generate_companion_response, APIResponse
    from search_engine_wrapper import search_wrapper, SearchResult
    from response_cache import response_cache
    LEGACY_AVAILABLE = True
    logger.info("âœ… Legacy components loaded")
except ImportError as e:
    LEGACY_AVAILABLE = False
    # This is expected - new system uses phases instead of legacy components
    logger.debug(f"Legacy components not loaded (using new phases): {e}")
finally:
    # Remove website from path to avoid conflicts
    if website_dir in sys.path:
        sys.path.remove(website_dir)

# Import Bytez client
try:
    from core.bytez_client import BytezClient
    BYTEZ_AVAILABLE = True
except ImportError:
    BYTEZ_AVAILABLE = False
    logger.warning("âš ï¸  Bytez client not available")

# Import Groq client
try:
    from groq import Groq
    GROQ_AVAILABLE = True
    logger.info("âœ… Groq client loaded")
except ImportError:
    GROQ_AVAILABLE = False
    logger.warning("âš ï¸  Groq client not available")

# Import Thread Manager for centralized thread management
try:
    from core.thread_manager import ThreadManager, ThreadPriority, ThreadState, create_worker_thread
    THREAD_MANAGER_AVAILABLE = True
    logger.info("âœ… Thread Manager loaded")
except ImportError as e:
    THREAD_MANAGER_AVAILABLE = False
    logger.warning(f"âš ï¸  Thread Manager not available: {e}")

# ============================================================================
# ADVANCED FEATURES (8 Advanced Capabilities)
# ============================================================================
try:
    from core.advanced_brain_wrapper import AdvancedBrainWrapper
    from core.multimodal import MediaInput, ModalityType
    ADVANCED_FEATURES_AVAILABLE = True
    logger.info("âœ… Advanced Features loaded (8 capabilities)")
except ImportError as e:
    ADVANCED_FEATURES_AVAILABLE = False
    logger.warning(f"âš ï¸  Advanced Features not available: {e}")

class CompanionBrain:
    """
    Universal AI Brain that can power any application type
    
    Examples:
        # For a chatbot
        brain = CompanionBrain(app_type="chatbot")
        response = brain.think(message="Hello!", context={"user_id": "123"})
        
        # For a code assistant
        brain = CompanionBrain(app_type="coder")
        response = brain.think(message="Fix this bug", context={"code": code_snippet})
        
        # For an image generator
        brain = CompanionBrain(app_type="image_gen")
        response = brain.think(message="Generate image of sunset", context={"style": "realistic"})
    """
    
    def __init__(
        self,
        app_type: str = "general",
        config: Optional[Dict[str, Any]] = None,
        enable_caching: bool = True,
        enable_search: bool = True,
        enable_learning: bool = True,
        enable_agi: bool = False,
        enable_autonomy: bool = False
    ):
        """
        Initialize the Companion Brain
        
        Args:
            app_type: Type of application ("chatbot", "coder", "image_gen", "research", etc.)
            config: Custom configuration dictionary
            enable_caching: Enable response caching for faster responses
            enable_search: Enable web search capabilities
            enable_learning: Enable adaptive learning from feedback
            enable_agi: Enable AGI features (personality, neural reasoning, self-learning) [TIER 4]
            enable_autonomy: Enable autonomous capabilities (self-modification, decisions) [TIER 4 - Advanced]
        """
        self.app_type = app_type
        self.config = config or {}
        self.enable_caching = enable_caching
        self.enable_search = enable_search
        self.enable_learning = enable_learning
        self.enable_agi = enable_agi
        self.enable_autonomy = enable_autonomy
        
        # Initialize brain components
        self.session_id = str(uuid.uuid4())
        self.created_at = datetime.now()
        self.request_count = 0
        self.success_count = 0
        
        # Initialize AI providers
        self.providers = self._initialize_providers()
        
        # Context storage for conversations
        self.contexts = {}  # user_id -> conversation context
        
        # Initialize Phase Components
        self._initialize_phase1()  # Knowledge Layer
        self._initialize_phase2()  # Search Layer
        self._initialize_phase3()  # Web Intelligence
        self._initialize_phase4()  # Execution & Generation
        self._initialize_phase5()  # Optimization
        
        # Initialize Advanced Features (8 Advanced Capabilities)
        self._initialize_advanced_features()
        
        # Initialize AGI Features (Tier 4) - Optional
        self._initialize_agi_features()
        
        # Initialize Thread Manager - Centralized thread management for all modules
        self._initialize_thread_manager()
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'cached_responses': 0,
            'average_response_time': 0.0,
            'models_used': {},
            'phases_enabled': self._get_enabled_phases(),
            'advanced_features_enabled': hasattr(self, 'advanced') and self.advanced is not None,
            'agi_enabled': self.enable_agi,
            'autonomy_enabled': self.enable_autonomy,
            'thread_manager_enabled': hasattr(self, 'thread_manager') and self.thread_manager is not None
        }
        
        # Circuit breakers for phase components (Tier 2 reliability)
        self.circuit_breakers = {
            'bytez': CircuitBreaker(failure_threshold=5, recovery_timeout=120, name='bytez'),
            'elasticsearch': CircuitBreaker(failure_threshold=3, recovery_timeout=60, name='elasticsearch'),
            'meilisearch': CircuitBreaker(failure_threshold=3, recovery_timeout=60, name='meilisearch'),
            'web_crawler': CircuitBreaker(failure_threshold=5, recovery_timeout=90, name='web_crawler'),
            'code_executor': CircuitBreaker(failure_threshold=3, recovery_timeout=45, name='code_executor'),
        }
        
        # Tier 3: Advanced features
        self.semantic_cache = SemanticCache(similarity_threshold=0.85, max_size=1000)
        self.multi_model_consensus = MultiModelConsensus(self)
        self.prompt_optimizer = PromptOptimizer(max_variants=5, min_samples_per_variant=10)
        self.performance_monitor = PerformanceMonitor(max_samples=10000)
        
        # Register default prompt variants for common tasks
        self._register_default_prompt_variants()
        
        logger.info(f"ðŸ§  Companion Brain initialized for app_type='{app_type}' (session: {self.session_id[:8]})")
        logger.info(f"ðŸ“¦ Phases enabled: {', '.join(self._get_enabled_phases())}")
        logger.info(f"ðŸ›¡ï¸ Circuit breakers initialized: {len(self.circuit_breakers)} components protected")
        logger.info(f"ðŸŽ¯ Tier 3 features: Semantic cache={'âœ…' if self.semantic_cache.model else 'âš ï¸'}, Consensus=âœ…, Prompt optimizer=âœ…, Performance monitor=âœ…")
    
    def think(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None,
        tools: Optional[List[str]] = None,
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        use_agi_decision: bool = True
    ) -> Dict[str, Any]:
        """
        The main "thinking" method - processes any request and returns intelligent response
        
        **NEW: AGI Decision Engine Integration**
        When AGI is enabled and use_agi_decision=True, the brain autonomously:
        1. Analyzes the query type and complexity
        2. Decides which modules to use
        3. Creates an execution plan
        4. Orchestrates multi-module workflows
        5. Learns from outcomes
        
        Workflow:
            Query â†’ AGI Engine â†’ Analyzes â†’ Decides Modules â†’ Orchestrates â†’ Response
        
        Args:
            message: User's input message/query
            context: Application-specific context (user data, state, etc.)
            tools: List of tools to use ['web', 'code', 'think', 'research']
            user_id: Unique user identifier for context management
            conversation_id: Conversation ID for maintaining history
            use_agi_decision: Whether to use AGI Decision Engine (default: True if AGI enabled)
            
        Returns:
            Dict containing:
                - response: The generated response text
                - metadata: Additional information (model used, timing, modules used, etc.)
                - success: Boolean indicating success
                - error: Error message if failed
                - agi_plan: Decision plan if AGI engine was used
        """
        self.request_count += 1
        self.stats['total_requests'] += 1
        
        start_time = datetime.now()
        think_start = start_time  # Track think() latency
        
        # AGI DECISION ENGINE - Autonomous intelligence
        if use_agi_decision and self.enable_agi and self.agi_decision_engine:
            return self._think_with_agi(message, context, tools, user_id, conversation_id, start_time)
        
        # Legacy thinking path (fallback)
        
        try:
            # Get or create context for this user/conversation
            context_key = conversation_id or user_id or "default"
            if context_key not in self.contexts:
                self.contexts[context_key] = {
                    'history': [],
                    'metadata': context or {},
                    'created_at': datetime.now()
                }
            
            conversation_context = self.contexts[context_key]
            chat_history = conversation_context['history']
            
            # Merge provided context with stored context
            if context:
                conversation_context['metadata'].update(context)
            
            # Determine tools based on app type if not specified
            if tools is None:
                tools = self._get_default_tools_for_app_type()
            
            # Add message to history
            conversation_context['history'].append({
                'role': 'user',
                'content': message,
                'timestamp': datetime.now().isoformat()
            })

            # Manage context window to avoid token overflow (trim/summarize)
            try:
                self._manage_context_window(conversation_context)
            except Exception:
                # Non-fatal - continue even if context manager fails
                logger.debug("Context window management failed, continuing")
            
            # Tier 3: Check semantic cache first (higher priority)
            if self.enable_caching and self.semantic_cache.model:
                cache_context = {
                    'app_type': self.app_type,
                    'tools': sorted(tools),
                    'has_history': len(chat_history) > 1
                }
                cached = self.semantic_cache.get(message, cache_context)
                if cached:
                    self.stats['cached_responses'] += 1
                    logger.info(f"âš¡ Using semantic cached response")
                    
                    # Still add to history
                    conversation_context['history'].append({
                        'role': 'assistant',
                        'content': cached,
                        'timestamp': datetime.now().isoformat(),
                        'cached': True,
                        'cache_type': 'semantic'
                    })
                    
                    return {
                        'response': cached,
                        'metadata': {
                            'cached': True,
                            'cache_type': 'semantic',
                            'session_id': self.session_id,
                            'app_type': self.app_type,
                            'response_time': (datetime.now() - start_time).total_seconds()
                        },
                        'success': True
                    }
            
            # Fallback: Check legacy exact-match cache if enabled
            if self.enable_caching and response_cache is not None:
                cache_context = {
                    'app_type': self.app_type,
                    'tools': sorted(tools),
                    'has_history': len(chat_history) > 1
                }
                cached = response_cache.get(message, cache_context)
                if cached:
                    self.stats['cached_responses'] += 1
                    logger.info(f"âš¡ Using cached response")
                    
                    # Still add to history
                    conversation_context['history'].append({
                        'role': 'assistant',
                        'content': cached,
                        'timestamp': datetime.now().isoformat(),
                        'cached': True
                    })
                    
                    return {
                        'response': cached,
                        'metadata': {
                            'cached': True,
                            'session_id': self.session_id,
                            'app_type': self.app_type,
                            'response_time': (datetime.now() - start_time).total_seconds()
                        },
                        'success': True
                    }
            
            # Generate response using Companion API Wrapper
            logger.info(f"ðŸ§  Processing request for app_type='{self.app_type}' with tools={tools}")
            
            # Check if legacy components are available
            if generate_companion_response is None:
                # Use Bytez or Groq provider as fallback
                logger.info("Using Bytez provider for response generation")
                provider_result = self.use_bytez(message, task='chat')
                
                if not provider_result['success']:
                    logger.info("Bytez failed, trying Groq...")
                    provider_result = self.use_groq(message)
                
                if provider_result['success']:
                    response_content = provider_result['response']
                    model_used = provider_result.get('model', 'groq')
                else:
                    # Fallback to simple response
                    response_content = "I apologize, but I'm currently unable to process your request due to missing dependencies. Please check the system configuration."
                    model_used = 'fallback'
                
                # Add response to history
                conversation_context['history'].append({
                    'role': 'assistant',
                    'content': response_content,
                    'timestamp': datetime.now().isoformat(),
                    'model': model_used
                })
                
                response_time = (datetime.now() - start_time).total_seconds()
                
                return {
                    'response': response_content,
                    'metadata': {
                        'session_id': self.session_id,
                        'app_type': self.app_type,
                        'response_time': response_time,
                        'model': model_used,
                        'fallback': True
                    },
                    'success': True
                }
            
            api_response = generate_companion_response(
                message=message,
                tools=tools,
                chat_history=chat_history[:-1]  # Exclude the message we just added
            )
            
            if api_response.success:
                self.success_count += 1
                self.stats['successful_requests'] += 1
                
                # Track model usage
                model_name = api_response.model
                self.stats['models_used'][model_name] = self.stats['models_used'].get(model_name, 0) + 1
                
                # Add response to history
                conversation_context['history'].append({
                    'role': 'assistant',
                    'content': api_response.content,
                    'timestamp': datetime.now().isoformat(),
                    'model': model_name,
                    'source': api_response.source
                })
                
                # Cache the response in both caches
                cache_context = {
                    'app_type': self.app_type,
                    'tools': sorted(tools),
                    'has_history': len(chat_history) > 0
                }
                
                # Tier 3: Semantic cache (priority)
                if self.enable_caching and self.semantic_cache.model:
                    self.semantic_cache.set(message, api_response.content, cache_context)
                
                # Legacy exact-match cache
                if self.enable_caching and response_cache is not None:
                    response_cache.set(message, api_response.content, cache_context)
                
                # Calculate response time
                response_time = (datetime.now() - start_time).total_seconds()
                think_latency = (datetime.now() - think_start).total_seconds()
                
                # Update average response time
                total_time = self.stats['average_response_time'] * (self.stats['successful_requests'] - 1)
                self.stats['average_response_time'] = (total_time + response_time) / self.stats['successful_requests']
                
                # Tier 3: Record performance metrics
                self.performance_monitor.record('think_total', think_latency)
                self.performance_monitor.record('llm_call', response_time)
                self.performance_monitor.record(f'model_{model_name}', response_time)
                
                logger.info(f"âœ… Brain generated response in {response_time:.2f}s using {model_name}")
                
                return {
                    'response': api_response.content,
                    'metadata': {
                        'model': model_name,
                        'source': api_response.source,
                        'response_time': response_time,
                        'thinking_data': api_response.thinking_data,
                        'links': api_response.links,
                        'session_id': self.session_id,
                        'app_type': self.app_type,
                        'tools_used': tools
                    },
                    'success': True
                }
            else:
                self.stats['failed_requests'] += 1
                logger.error(f"âŒ Brain failed to generate response: {api_response.error}")
                
                return {
                    'response': f"I apologize, but I encountered an error: {api_response.error}",
                    'metadata': {
                        'session_id': self.session_id,
                        'app_type': self.app_type,
                        'error': api_response.error
                    },
                    'success': False,
                    'error': api_response.error
                }
                
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ Brain error: {str(e)}")
            
            return {
                'response': "I encountered an unexpected error while processing your request.",
                'metadata': {
                    'session_id': self.session_id,
                    'app_type': self.app_type,
                    'error': str(e)
                },
                'success': False,
                'error': str(e)
            }
    
    def _think_with_agi(self, message: str, context: Optional[Dict[str, Any]], 
                       tools: Optional[List[str]], user_id: Optional[str], 
                       conversation_id: Optional[str], start_time: datetime) -> Dict[str, Any]:
        """
        AGI-powered thinking - Autonomous decision-making and execution
        
        The AGI Decision Engine autonomously:
        1. Analyzes the query
        2. Decides which modules to use
        3. Creates execution plan
        4. Executes the plan
        5. Learns from the outcome
        """
        logger.info("ðŸ¤– Using AGI Decision Engine for autonomous processing")
        
        try:
            # Get or create context
            context_key = conversation_id or user_id or "default"
            if context_key not in self.contexts:
                self.contexts[context_key] = {
                    'history': [],
                    'metadata': context or {},
                    'created_at': datetime.now()
                }
            
            conversation_context = self.contexts[context_key]
            
            # Merge context
            if context:
                conversation_context['metadata'].update(context)
            
            # Add message to history
            conversation_context['history'].append({
                'role': 'user',
                'content': message,
                'timestamp': datetime.now().isoformat()
            })
            
            # Step 1: AGI analyzes query and makes decision
            decision_start = datetime.now()
            decision_plan = self.agi_decision_engine.analyze_and_decide(
                query=message,
                context=conversation_context['metadata']
            )
            decision_time = (datetime.now() - decision_start).total_seconds()
            
            logger.info(f"âœ… AGI decided to use {len(decision_plan.modules_to_use)} modules "
                       f"with {decision_plan.confidence:.1%} confidence")
            logger.debug(f"   Query type: {decision_plan.query_type.value}")
            logger.debug(f"   Execution plan: {len(decision_plan.execution_order)} steps")
            
            # Step 2: AGI executes the decision plan
            execution_start = datetime.now()
            execution_result = self.agi_decision_engine.execute_decision(
                plan=decision_plan,
                query=message,
                context=conversation_context['metadata']
            )
            execution_time = (datetime.now() - execution_start).total_seconds()
            
            # Calculate total time
            total_time = (datetime.now() - start_time).total_seconds()
            
            if execution_result.success:
                self.success_count += 1
                self.stats['successful_requests'] += 1
                
                # Add response to history
                conversation_context['history'].append({
                    'role': 'assistant',
                    'content': str(execution_result.response),
                    'timestamp': datetime.now().isoformat(),
                    'agi_powered': True,
                    'modules_used': execution_result.modules_used,
                    'decision_id': decision_plan.decision_id
                })
                
                # Update average response time
                total_time_sum = self.stats['average_response_time'] * (self.stats['successful_requests'] - 1)
                self.stats['average_response_time'] = (total_time_sum + total_time) / self.stats['successful_requests']
                
                logger.info(f"âœ… AGI execution completed: {execution_result.steps_completed}/{len(decision_plan.execution_order)} steps "
                           f"in {total_time:.2f}s")
                
                return {
                    'response': str(execution_result.response),
                    'metadata': {
                        'agi_powered': True,
                        'decision_id': decision_plan.decision_id,
                        'query_type': decision_plan.query_type.value,
                        'confidence': decision_plan.confidence,
                        'modules_used': execution_result.modules_used,
                        'steps_completed': execution_result.steps_completed,
                        'execution_time': execution_time,
                        'decision_time': decision_time,
                        'total_time': total_time,
                        'reasoning': decision_plan.reasoning,
                        'learned_insights': execution_result.learned_insights,
                        'session_id': self.session_id,
                        'app_type': self.app_type
                    },
                    'success': True,
                    'agi_plan': {
                        'decision_id': decision_plan.decision_id,
                        'query_type': decision_plan.query_type.value,
                        'modules_to_use': [m.value for m in decision_plan.modules_to_use],
                        'execution_order': decision_plan.execution_order,
                        'confidence': decision_plan.confidence,
                        'reasoning': decision_plan.reasoning
                    }
                }
            else:
                self.stats['failed_requests'] += 1
                error_msg = "; ".join(execution_result.errors) if execution_result.errors else "Unknown error"
                
                logger.error(f"âŒ AGI execution partial success: {execution_result.steps_completed}/{len(decision_plan.execution_order)} steps, "
                            f"errors: {error_msg}")
                
                return {
                    'response': str(execution_result.response) if execution_result.response else "I encountered difficulties processing your request.",
                    'metadata': {
                        'agi_powered': True,
                        'decision_id': decision_plan.decision_id,
                        'steps_completed': execution_result.steps_completed,
                        'steps_planned': len(decision_plan.execution_order),
                        'partial_success': execution_result.steps_completed > 0,
                        'errors': execution_result.errors,
                        'session_id': self.session_id
                    },
                    'success': execution_result.steps_completed > 0,  # Partial success
                    'error': error_msg
                }
                
        except Exception as e:
            self.stats['failed_requests'] += 1
            logger.error(f"âŒ AGI thinking error: {str(e)}")
            import traceback
            logger.debug(traceback.format_exc())
            
            # Fallback to legacy thinking
            logger.info("âš ï¸ Falling back to legacy thinking mode")
            return self._think_legacy(message, context, tools, user_id, conversation_id, start_time)
    
    def _think_legacy(self, message: str, context: Optional[Dict[str, Any]], 
                     tools: Optional[List[str]], user_id: Optional[str], 
                     conversation_id: Optional[str], start_time: datetime) -> Dict[str, Any]:
        """Legacy thinking method (original implementation)"""
        try:
            # Get or create context for this user/conversation
            context_key = conversation_id or user_id or "default"
            if context_key not in self.contexts:
                self.contexts[context_key] = {
                    'history': [],
                    'metadata': context or {},
                    'created_at': datetime.now()
                }
            
            conversation_context = self.contexts[context_key]
            chat_history = conversation_context['history']
            
            # Merge provided context with stored context
            if context:
                conversation_context['metadata'].update(context)
            
            # Determine tools based on app type if not specified
            if tools is None:
                tools = self._get_default_tools_for_app_type()
            
            # Add message to history
            conversation_context['history'].append({
                'role': 'user',
                'content': message,
                'timestamp': datetime.now().isoformat()
            })

            # Manage context window to avoid token overflow (trim/summarize)
            try:
                self._manage_context_window(conversation_context)
            except Exception:
                # Non-fatal - continue even if context manager fails
                logger.debug("Context window management failed, continuing")
            
            # Continue with legacy response generation...
            # [Rest of the original think() implementation]
            response = self._call_llm(message, conversation_context['metadata'])
            
            conversation_context['history'].append({
                'role': 'assistant',
                'content': response,
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'response': response,
                'metadata': {
                    'session_id': self.session_id,
                    'response_time': (datetime.now() - start_time).total_seconds()
                },
                'success': True
            }
            
        except Exception as e:
            logger.error(f"âŒ Legacy thinking error: {str(e)}")
            return {
                'response': "I encountered an error processing your request.",
                'metadata': {'error': str(e)},
                'success': False,
                'error': str(e)
            }
    
    def _get_default_tools_for_app_type(self) -> List[str]:
        """Get default tools based on application type"""
        tool_mappings = {
            'chatbot': [],  # General conversation, no special tools
            'coder': ['code'],  # Code-focused
            'research': ['web', 'deepsearch'],  # Research with web search
            'image_gen': [],  # Image generation (handled by specialized models)
            'video_gen': [],  # Video generation
            'assistant': ['web'],  # General assistant with web access
            'tutor': ['web'],  # Educational tutor with web access
            'analyst': ['deepsearch', 'think'],  # Data analysis with deep thinking
            'general': []  # Default
        }
        
        return tool_mappings.get(self.app_type, [])
    
    def _initialize_providers(self) -> Dict[str, Any]:
        """Initialize all AI providers including Bytez"""
        providers = {}
        
        # Initialize Bytez if available
        if BYTEZ_AVAILABLE:
            try:
                api_key = os.getenv('BYTEZ_API_KEY')
                if not api_key:
                    raise ValueError("BYTEZ_API_KEY environment variable is required")
                providers['bytez'] = BytezClient(api_key=api_key)
                logger.info("âœ… Bytez provider initialized (141k+ models)")
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to initialize Bytez: {e}")
        
        # Initialize Groq
        if GROQ_AVAILABLE:
            try:
                groq_api_key = os.getenv('GROQ_API_KEY')
                if not groq_api_key:
                    raise ValueError("GROQ_API_KEY environment variable is required")
                providers['groq'] = Groq(api_key=groq_api_key)
                logger.info("âœ… Groq provider initialized")
            except Exception as e:
                logger.warning(f"âš ï¸  Failed to initialize Groq: {e}")
        
        # Could add other providers here in the future
        # providers['ollama'] = OllamaClient()
        
        return providers
    
    def use_bytez(
        self,
        message: str,
        model: Optional[str] = None,
        task: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Use Bytez directly for simple queries (free tier optimization)
        
        Args:
            message: User message
            model: Specific Bytez model to use
            task: Task type ('chat', 'code', 'reasoning', 'general')
        
        Returns:
            Dict with response and metadata
        """
        if 'bytez' not in self.providers:
            return {
                'success': False,
                'error': 'Bytez provider not available',
                'response': None
            }
        
        try:
            bytez = self.providers['bytez']
            
            # Select model based on task if not specified
            if not model and task:
                recommended = bytez.list_recommended_models()
                model = recommended.get(task, recommended.get('chat'))
            
            start_time = datetime.now()
            
            # Generate response
            # If model not provided, route to best model for the task/message
            if not model:
                model = self._route_to_best_model(message=message, task=task)

            # Prepare callable for retry wrapper
            def _call():
                return bytez.generate(
                    messages=[{'role': 'user', 'content': message}],
                    model=model,
                    max_tokens=1024  # Set reasonable token limit like Groq
                )

            result = self._call_with_retry(_call, provider='bytez', model=model)
            
            response_time = (datetime.now() - start_time).total_seconds()
            
            if result['success']:
                self.stats['successful_requests'] += 1
                
                # Track Bytez usage
                if 'bytez' not in self.stats['models_used']:
                    self.stats['models_used']['bytez'] = 0
                self.stats['models_used']['bytez'] += 1
                
                logger.info(f"âœ… Bytez response in {response_time:.2f}s using {result['model']}")
                
                return {
                    'success': True,
                    'response': result['response'],
                    'metadata': {
                        'provider': 'bytez',
                        'model': result['model'],
                        'response_time': response_time,
                        'free_tier': True
                    }
                }
            else:
                # Bytez failed
                logger.warning(f"âš ï¸ Bytez failed: {result.get('error')}")
                return result  # Return Bytez error
        except Exception as e:
            logger.error(f"âŒ Bytez error: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': None
            }
    
    def use_groq(self, message: str) -> Dict[str, Any]:
        """
        Use Groq for chat completion
        
        Args:
            message: User message
            
        Returns:
            Dict with response and metadata
        """
        if 'groq' not in self.providers:
            return {
                'success': False,
                'error': 'Groq provider not available',
                'response': None
            }
        
        try:
            groq_client = self.providers['groq']
            start_time = datetime.now()
            
            chat_completion = groq_client.chat.completions.create(
                messages=[{"role": "user", "content": message}],
                model="llama3-8b-8192",  # Fast and good model
                max_tokens=1024,
                temperature=0.7
            )
            
            response_content = chat_completion.choices[0].message.content
            response_time = (datetime.now() - start_time).total_seconds()
            
            self.stats['successful_requests'] += 1
            if 'groq' not in self.stats['models_used']:
                self.stats['models_used']['groq'] = 0
            self.stats['models_used']['groq'] += 1
            
            logger.info(f"âœ… Groq response in {response_time:.2f}s")
            
            return {
                'success': True,
                'response': response_content,
                'model': 'llama3-8b-8192',
                'metadata': {
                    'provider': 'groq',
                    'response_time': response_time,
                    'tokens': chat_completion.usage.total_tokens if hasattr(chat_completion, 'usage') else None
                }
            }
        except Exception as e:
            logger.error(f"âŒ Groq error: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': None
            }
    
    def get_conversation_history(
        self,
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        limit: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Get conversation history for a user/conversation"""
        context_key = conversation_id or user_id or "default"
        
        if context_key not in self.contexts:
            return []
        
        history = self.contexts[context_key]['history']
        
        if limit:
            return history[-limit:]
        
        return history
    
    def clear_conversation(
        self,
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None
    ):
        """Clear conversation history"""
        context_key = conversation_id or user_id or "default"
        
        if context_key in self.contexts:
            self.contexts[context_key]['history'] = []
            logger.info(f"ðŸ§¹ Cleared conversation for {context_key}")

    # ------------------------------------------------------------------
    # Tier-1 helpers: retry, context management, model routing, metrics
    # ------------------------------------------------------------------
    def _call_with_retry(self, func: Callable, max_retries: int = 3, backoff_factor: float = 1.5, 
                        provider: Optional[str] = None, model: Optional[str] = None, 
                        use_circuit_breaker: bool = True):
        """
        Call a synchronous function with retries, exponential backoff, and circuit breaker protection.
        Returns the successful function result or raises the last exception.
        Also records basic metrics into self.stats when provider/model provided.
        
        Args:
            func: Callable to execute
            max_retries: Maximum retry attempts
            backoff_factor: Exponential backoff multiplier
            provider: Provider name for metrics and circuit breaker
            model: Model name for metrics
            use_circuit_breaker: Enable circuit breaker protection
        """
        # Wrap with circuit breaker if enabled and available
        if use_circuit_breaker and provider and provider in self.circuit_breakers:
            breaker = self.circuit_breakers[provider]
            # Check if circuit is open
            if breaker.state == CircuitState.OPEN:
                logger.warning(f"âš ï¸ Circuit breaker for '{provider}' is OPEN, skipping request")
                raise RuntimeError(f"Circuit breaker '{provider}' is OPEN")
        
        attempt = 0
        last_exc = None
        while attempt < max_retries:
            try:
                start = time.time()
                
                # Execute with circuit breaker protection if enabled
                if use_circuit_breaker and provider and provider in self.circuit_breakers:
                    result = self.circuit_breakers[provider].call(func)
                else:
                    result = func()
                    
                elapsed = time.time() - start

                # record latency per provider
                if provider:
                    key = f"latency_{provider}"
                    self.stats.setdefault('phase_latencies', {})
                    self.stats['phase_latencies'].setdefault(key, [])
                    self.stats['phase_latencies'][key].append(elapsed)

                # model usage
                if model:
                    self.stats.setdefault('models_used', {})
                    self.stats['models_used'].setdefault(model, 0)
                    self.stats['models_used'][model] += 1

                return result
            except Exception as e:
                last_exc = e
                wait = backoff_factor * (2 ** attempt)
                logger.warning(f"Retry {attempt+1}/{max_retries} failed for provider={provider} model={model}: {e}; retrying in {wait:.1f}s")
                time.sleep(wait)
                attempt += 1

        # exhausted retries
        logger.error(f"All retries failed for provider={provider} model={model}: {last_exc}")
        if last_exc:
            raise last_exc
        raise RuntimeError(f"Retries exhausted for provider={provider} model={model}")

    def _manage_context_window(self, conversation_context: Dict[str, Any], max_turns: int = 40, use_llm_summary: bool = True):
        """
        Trim or summarize conversation history to avoid token limits.
        If history longer than max_turns, compress older turns into an intelligent
        LLM-generated summary to preserve context while saving tokens.
        
        Args:
            conversation_context: Conversation context dict with 'history' key
            max_turns: Maximum number of turns to keep
            use_llm_summary: If True, use LLM to create intelligent summary; else use simple placeholder
        """
        history = conversation_context.get('history', [])
        if len(history) <= max_turns:
            return

        # Keep the most recent turns and summarize the older ones
        keep = int(max_turns * 0.6)
        recent = history[-keep:]
        older = history[:-keep]

        # Create intelligent summary using LLM if available
        if use_llm_summary and len(older) > 2:
            try:
                # Build condensed conversation string from older messages
                older_text = "\n".join([
                    f"{msg.get('role', 'unknown')}: {msg.get('content', '')[:200]}"
                    for msg in older[:20]  # Limit to first 20 for summarization
                ])
                
                summary_prompt = f"""Summarize the following conversation history in 2-3 sentences, capturing key topics, decisions, and context:

{older_text}

Summary:"""
                
                # Use _call_llm with retry wrapper for robust summarization
                summary_response = self._call_llm(summary_prompt)
                summary_text = f"[Previous conversation context: {summary_response}]"
                
                logger.info(f"âœ… Summarized {len(older)} messages using LLM")
            except Exception as e:
                logger.warning(f"âš ï¸ LLM summarization failed: {e}, using placeholder")
                summary_text = f"[Summarized {len(older)} earlier messages to save tokens]"
        else:
            # Fallback: simple placeholder
            summary_text = f"[Summarized {len(older)} earlier messages to save tokens]"

        summary_entry = {
            'role': 'system',
            'content': summary_text,
            'timestamp': datetime.now().isoformat(),
            'summarized_count': len(older)
        }

        # Replace history with summary + recent
        conversation_context['history'] = [summary_entry] + recent
        logger.debug(f"ðŸ“Š Context window managed: {len(older)} â†’ 1 summary + {len(recent)} recent turns")

    def _estimate_tokens(self, text: str) -> int:
        """
        Quick token estimation using simple heuristic: ~4 chars per token.
        Good enough for context window management without loading tokenizer.
        """
        if not text:
            return 0
        return len(text) // 4
    
    def _route_to_best_model(self, message: str, task: Optional[str] = None, estimated_tokens: Optional[int] = None) -> Optional[str]:
        """
        Intelligent heuristic-based model router.
        Selects specialized models based on task type, content patterns, and context size.
        Returns a model name compatible with Bytez or None to let provider choose.
        
        Args:
            message: Input message to analyze
            task: Optional explicit task type hint
            estimated_tokens: Optional pre-computed token count
            
        Returns:
            Model name string or None
        """
        try:
            text = (message or "").lower()
            tokens = estimated_tokens or self._estimate_tokens(message)

            # Task hints override heuristics
            if task:
                if 'code' in task:
                    return 'deepseek-coder'
                if 'reason' in task or 'think' in task:
                    return 'qwen-4b'  # Changed from phi-2-reasoner (Bytez too slow)
                if 'chat' in task:
                    return 'qwen-4b'

            # Token-based routing for long context
            if tokens > 4000:
                # Very long context -> use model with large context window
                logger.debug(f"ðŸ“ Long context detected ({tokens} tokens) â†’ using qwen-72b")
                return 'qwen-72b'
            elif tokens > 2000:
                # Medium-long context
                return 'qwen-32b'

            # Content pattern heuristics
            code_patterns = ['def ', 'import ', 'class ', 'function ', 'const ', 'var ', 
                           'console.log', 'print(', 'return ', '```', 'async ', 'await ']
            if any(k in text for k in code_patterns):
                logger.debug("ðŸ’» Code pattern detected â†’ using deepseek-coder")
                return 'deepseek-coder'

            reasoning_patterns = ['why', 'how', 'because', 'explain', 'prove', 'analyze',
                                'compare', 'evaluate', 'reason', 'logic']
            if any(k in text for k in reasoning_patterns):
                logger.debug("ðŸ§  Reasoning pattern detected â†’ using qwen-4b")
                return 'qwen-4b'  # Changed from phi-2-reasoner (Bytez too slow)

            math_patterns = ['calculate', 'compute', 'equation', 'formula', 'solve', 
                           'integral', 'derivative', 'theorem']
            if any(k in text for k in math_patterns):
                logger.debug("ðŸ”¢ Math pattern detected â†’ using qwen-math")
                return 'qwen-math'

            # Default: fast general chat model
            logger.debug("ðŸ’¬ General chat â†’ using qwen-4b")
            return 'qwen-4b'
        except Exception as e:
            logger.warning(f"âš ï¸ Model routing failed: {e}, using default")
            return None
    
    # Tier 3: Multi-model consensus public API
    async def query_with_consensus(
        self, 
        message: str,
        models: Optional[List[str]] = None,
        min_agreement: float = 0.6,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Query multiple models in parallel and combine results with consensus scoring.
        
        This is useful for critical queries where you want higher confidence by
        cross-validating responses from multiple models.
        
        Args:
            message: The user message/query
            models: List of model names to query (default: ['qwen-4b', 'phi-2-reasoner', 'deepseek-coder'])
            min_agreement: Minimum agreement threshold for consensus (0.0-1.0)
            context: Optional conversation context dict
            
        Returns:
            Dict with keys:
                - response: Combined response text
                - confidence: Overall confidence score (0.0-1.0)
                - models_used: List of models that were queried
                - individual_results: List of individual model responses
                - agreement_score: Agreement level between models (0.0-1.0)
                - cache_hit: Whether result was from semantic cache
                
        Example:
            result = await brain.query_with_consensus(
                "Explain quantum entanglement",
                models=['qwen-72b', 'phi-2-reasoner', 'deepseek-coder'],
                min_agreement=0.7
            )
            print(f"Response: {result['response']}")
            print(f"Confidence: {result['confidence']:.2%}")
            print(f"Agreement: {result['agreement_score']:.2%}")
        """
        try:
            # Check semantic cache first for the consensus query
            cache_context = f"consensus_{models or 'default'}_{min_agreement}"
            cached = self.semantic_cache.get(message, cache_context)
            if cached:
                logger.info("âš¡ Using cached consensus result")
                return {
                    'response': cached,
                    'confidence': 0.95,  # High confidence for cached results
                    'models_used': models or ['qwen-4b', 'phi-2-reasoner', 'deepseek-coder'],
                    'individual_results': [],
                    'agreement_score': 1.0,
                    'cache_hit': True
                }
            
            # Query models with consensus
            result = await self.multi_model_consensus.query_with_consensus(
                message, 
                models=models, 
                min_agreement=min_agreement
            )
            
            # Cache the consensus result
            if result['response']:
                self.semantic_cache.set(message, result['response'], cache_context)
            
            result['cache_hit'] = False
            return result
            
        except Exception as e:
            logger.error(f"âŒ Consensus query failed: {e}")
            # Fallback to regular think() if consensus fails
            fallback = self.think(message, context or {})
            return {
                'response': fallback.get('response', ''),
                'confidence': 0.5,
                'models_used': [fallback.get('model', 'unknown')],
                'individual_results': [],
                'agreement_score': 0.0,
                'cache_hit': False,
                'error': str(e)
            }
    
    def get_consensus_stats(self) -> Dict[str, Any]:
        """
        Get statistics for multi-model consensus queries.
        
        Returns:
            Dict with keys:
                - consensus_queries: Total number of consensus queries
                - total_models_queried: Total models queried across all consensus calls
                - avg_models_per_query: Average number of models per consensus query
        """
        return self.multi_model_consensus.get_stats()
    
    def get_semantic_cache_stats(self) -> Dict[str, Any]:
        """
        Get statistics for semantic cache performance.
        
        Returns:
            Dict with keys:
                - hits: Number of cache hits
                - misses: Number of cache misses
                - hit_rate: Hit rate percentage (0.0-100.0)
                - cache_size: Number of entries in cache
        """
        return self.semantic_cache.get_stats()
    
    def optimize_prompt(
        self,
        task_type: str,
        task_params: Dict[str, Any],
        exploration_rate: float = 0.2
    ) -> str:
        """
        Get optimized prompt for a task using A/B testing results.
        
        This method selects the best-performing prompt variant based on historical
        performance data, using an epsilon-greedy strategy (exploration vs exploitation).
        
        Args:
            task_type: Type of task ('code_generation', 'explanation', 'debugging', etc.)
            task_params: Parameters to fill into prompt template (e.g., {'task': 'sort array'})
            exploration_rate: Probability of trying non-best variants (0.0-1.0, default: 0.2)
            
        Returns:
            Optimized prompt string ready to use
            
        Example:
            prompt = brain.optimize_prompt(
                'code_generation',
                {'task': 'implement binary search'},
                exploration_rate=0.1
            )
            response = brain.think(prompt, {})
        """
        variant_id = self.prompt_optimizer.select_variant(task_type, exploration_rate)
        
        if not variant_id:
            # No variants registered, return basic prompt
            logger.warning(f"âš ï¸ No prompt variants for task: {task_type}")
            return str(task_params.get('task') or task_params.get('topic') or str(task_params))
        
        # Get prompt template
        template = self.prompt_optimizer.get_variant_prompt(task_type, variant_id)
        if not template:
            return str(task_params.get('task') or task_params.get('topic') or str(task_params))
        
        # Fill in parameters
        try:
            prompt = template.format(**task_params)
            logger.debug(f"ðŸ“ Using prompt variant: {task_type}/{variant_id}")
            return prompt
        except KeyError as e:
            logger.warning(f"âš ï¸ Missing parameter for prompt template: {e}")
            return template
    
    def record_prompt_result(
        self,
        task_type: str,
        variant_id: str,
        success: bool,
        latency: float,
        response_length: int = 0
    ):
        """
        Record the result of using a prompt variant for continuous optimization.
        
        Call this after getting a response to help the optimizer learn which
        prompts work best for different tasks.
        
        Args:
            task_type: Type of task
            variant_id: Variant that was used
            success: Whether the response was successful/satisfactory
            latency: Response time in seconds
            response_length: Length of response in characters
            
        Example:
            start = time.time()
            response = brain.think(prompt, {})
            latency = time.time() - start
            
            brain.record_prompt_result(
                'code_generation',
                'detailed',
                success=True,
                latency=latency,
                response_length=len(response['response'])
            )
        """
        self.prompt_optimizer.record_result(
            task_type,
            variant_id,
            success,
            latency,
            response_length
        )
    
    def get_prompt_optimization_stats(self, task_type: Optional[str] = None) -> Dict[str, Any]:
        """
        Get statistics about prompt optimization experiments.
        
        Args:
            task_type: Optional task type to filter stats, or None for global stats
            
        Returns:
            Statistics dict with:
                - For specific task: variant performance, best variant
                - For global: total experiments, all task types, best variants
                
        Example:
            # Get stats for specific task
            stats = brain.get_prompt_optimization_stats('code_generation')
            print(f"Best variant: {stats['best_variant']}")
            
            # Get global stats
            global_stats = brain.get_prompt_optimization_stats()
            print(f"Total experiments: {global_stats['total_experiments']}")
        """
        return self.prompt_optimizer.get_stats(task_type)
    
    def get_performance_percentiles(self, metric_name: str) -> Dict[str, float]:
        """
        Get P50, P95, P99 percentiles for a performance metric.
        
        Args:
            metric_name: Name of the metric (e.g., 'think_total', 'llm_call', 'model_qwen-4b')
            
        Returns:
            Dict with p50, p95, p99, min, max, mean, count
            
        Example:
            percentiles = brain.get_performance_percentiles('think_total')
            print(f"P50: {percentiles['p50']:.3f}s")
            print(f"P95: {percentiles['p95']:.3f}s")
            print(f"P99: {percentiles['p99']:.3f}s")
        """
        return self.performance_monitor.get_percentiles(metric_name)
    
    def get_latency_histogram(self, metric_name: str, num_buckets: int = 10) -> Dict[str, Any]:
        """
        Get latency histogram for a performance metric.
        
        Args:
            metric_name: Name of the metric
            num_buckets: Number of histogram buckets (default: 10)
            
        Returns:
            Dict with buckets, counts, and total
            
        Example:
            histogram = brain.get_latency_histogram('llm_call', num_buckets=5)
            for bucket, count in zip(histogram['buckets'], histogram['counts']):
                print(f"{bucket}: {'#' * count}")
        """
        return self.performance_monitor.get_histogram(metric_name, num_buckets)
    
    def get_performance_monitoring_stats(self) -> Dict[str, Any]:
        """
        Get comprehensive Tier 3 performance monitoring statistics.
        
        Returns:
            Dict with percentiles and metrics for all tracked operations
            
        Example:
            perf_stats = brain.get_performance_monitoring_stats()
            print(f"Total metrics tracked: {perf_stats['total_metrics']}")
            for metric, data in perf_stats['metrics'].items():
                print(f"{metric}: P95={data['percentiles']['p95']:.3f}s")
        """
        return self.performance_monitor.get_stats()
    
    def get_stats(self) -> Dict[str, Any]:
        """Get brain statistics"""
        stats = {
            **self.stats,
            'session_id': self.session_id,
            'app_type': self.app_type,
            'uptime_seconds': (datetime.now() - self.created_at).total_seconds(),
            'success_rate': (self.success_count / self.request_count * 100) if self.request_count > 0 else 0.0,
            'active_conversations': len(self.contexts)
        }

        # Compute simple averages for phase latencies if available
        lat = stats.get('phase_latencies', {})
        avg_lat = {}
        for k, arr in lat.items():
            try:
                avg_lat[k] = sum(arr) / len(arr) if arr else 0.0
            except Exception:
                avg_lat[k] = 0.0

        stats['phase_latency_averages'] = avg_lat
        
        # Add circuit breaker states
        stats['circuit_breakers'] = {
            name: breaker.get_state() 
            for name, breaker in self.circuit_breakers.items()
        }
        
        # Tier 3: Add semantic cache, consensus, prompt optimization, and performance monitoring stats
        stats['semantic_cache'] = self.semantic_cache.get_stats()
        stats['multi_model_consensus'] = self.multi_model_consensus.get_stats()
        stats['prompt_optimizer'] = self.prompt_optimizer.get_stats()
        stats['performance_monitor'] = self.performance_monitor.get_stats()

        return stats
    
    def search_web(self, query: str, deep_search: bool = False) -> Dict[str, Any]:
        """Direct web search capability"""
        try:
            results = search_wrapper.enhanced_search_with_mining(query, deep_search=deep_search)
            return {
                'success': True,
                'results': results
            }
        except Exception as e:
            logger.error(f"Web search error: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def provide_feedback(
        self,
        message_id: str,
        rating: int,
        comment: Optional[str] = None,
        conversation_id: Optional[str] = None
    ):
        """Provide feedback for adaptive learning"""
        if not self.enable_learning:
            return
        
        # TODO: Implement adaptive learning from feedback
        logger.info(f"ðŸ“Š Feedback received: rating={rating}, message_id={message_id}")
    
    # ============================================================================
    # PHASE INITIALIZATION METHODS
    # ============================================================================
    
    def _initialize_phase1(self):
        """Initialize Phase 1: Knowledge Layer"""
        if not PHASE1_AVAILABLE:
            self.knowledge_retriever = None
            self.vector_store = None
            self.elasticsearch = None
            return
        
        try:
            self.elasticsearch = ElasticsearchClient()
            self.vector_store = VectorStore()
            self.knowledge_retriever = KnowledgeRetriever()
            logger.info("âœ… Phase 1 initialized: Knowledge Layer ready")
        except Exception as e:
            logger.warning(f"âš ï¸  Phase 1 initialization failed: {e}")
            self.knowledge_retriever = None
            self.vector_store = None
            self.elasticsearch = None
    
    def _initialize_phase2(self):
        """Initialize Phase 2: Search Layer"""
        if not PHASE2_AVAILABLE:
            self.search_engine = None
            self.meilisearch = None
            return
        
        try:
            self.meilisearch = MeilisearchClient()
            self.search_engine = SearchEngine()
            logger.info("âœ… Phase 2 initialized: Search Engine ready")
        except Exception as e:
            logger.warning(f"âš ï¸  Phase 2 initialization failed: {e}")
            self.search_engine = None
            self.meilisearch = None
    
    def _initialize_phase3(self):
        """Initialize Phase 3: Web Intelligence"""
        if not PHASE3_AVAILABLE:
            self.web_crawler = None
            self.news_api = None
            self.search_api = None
            return
        
        try:
            self.web_crawler = WebContentCrawler()  # Fixed: Use WebContentCrawler
            self.news_api = NewsAPIClient()
            self.search_api = WebSearchClient()  # Fixed: Use WebSearchClient
            logger.info("âœ… Phase 3 initialized: Web Intelligence ready")
        except Exception as e:
            logger.warning(f"âš ï¸  Phase 3 initialization failed: {e}")
            self.web_crawler = None
            self.news_api = None
            self.search_api = None
    
    def _initialize_phase4(self):
        """Initialize Phase 4: Execution & Generation"""
        if not PHASE4_AVAILABLE:
            self.code_executor = None
            self.tool_registry = None
            self.tool_executor = None
            return
        
        try:
            self.code_executor = CodeExecutor()
            self.tool_registry = ToolRegistry()
            register_builtin_tools(self.tool_registry)
            self.tool_executor = ToolExecutor(self.tool_registry)
            logger.info(f"âœ… Phase 4 initialized: Code execution + {len(self.tool_registry.list_tools())} tools ready")
        except Exception as e:
            logger.warning(f"âš ï¸  Phase 4 initialization failed: {e}")
            self.code_executor = None
            self.tool_registry = None
            self.tool_executor = None
    
    def _initialize_phase5(self):
        """Initialize Phase 5: Optimization"""
        if not PHASE5_AVAILABLE:
            self.profiler = None
            self.cache_optimizer = None
            self.monitor = None
            return
        
        try:
            self.profiler = profiler
            self.cache_optimizer = cache_optimizer
            self.monitor = monitor
            logger.info("âœ… Phase 5 initialized: Optimization & monitoring ready")
        except Exception as e:
            logger.warning(f"âš ï¸  Phase 5 initialization failed: {e}")
            self.profiler = None
            self.cache_optimizer = None
            self.monitor = None
    
    def _initialize_advanced_features(self):
        """Initialize Advanced Brain Features (8 capabilities)"""
        if not ADVANCED_FEATURES_AVAILABLE:
            self.advanced = None
            logger.debug("Advanced features not available")
            return
        
        try:
            logger.debug("Starting advanced features initialization...")
            # Create LLM function wrapper for advanced features
            def llm_wrapper(prompt: str) -> str:
                """Wrapper to make brain's LLM callable for advanced features"""
                result = self._call_llm(prompt)
                return result if isinstance(result, str) else str(result)
            
            # Initialize advanced brain with Bytez integration
            config = {
                'bytez_available': BYTEZ_AVAILABLE,
                'bytez_client': self.providers.get('bytez') if BYTEZ_AVAILABLE else None
            }
            
            logger.debug("Creating AdvancedBrainWrapper...")
            self.advanced = AdvancedBrainWrapper(llm_wrapper, config)
            logger.info("âœ… Advanced Features initialized with 8 capabilities")
        except Exception as e:
            import traceback
            logger.error(f"âŒ Advanced features initialization failed: {e}")
            logger.debug(traceback.format_exc())
            self.advanced = None
    
    def _initialize_agi_features(self):
        """Initialize AGI Features (Tier 4) - Personality, Neural Reasoning, Self-Learning, Autonomy"""
        # Initialize AGI components as None by default
        self.local_intelligence = None
        self.neural_reasoning = None
        self.personality_engine = None
        self.self_learning = None
        self.autonomous_system = None
        self.agi_decision_engine = None  # The autonomous decision-making core
        
        if not self.enable_agi:
            logger.debug("AGI features disabled")
            return
        
        try:
            # Import AGI modules from tier4 wrapper package
            from companion_baas.tier4 import (
                LocalIntelligenceCore,
                NeuralReasoningEngine,
                PersonalityEngine,
                SelfLearningSystem,
                AGI_AVAILABLE
            )
            
            if not AGI_AVAILABLE:
                logger.warning("âš ï¸ AGI components not fully available - some features disabled")
                # Don't disable AGI completely, just warn
                # self.enable_agi = False
                # return
            
            logger.debug("ðŸ§  Initializing AGI components...")
            
            # Initialize Local Intelligence Core
            try:
                self.local_intelligence = LocalIntelligenceCore()
                logger.info("âœ… Local Intelligence Core initialized")
            except Exception as e:
                logger.warning(f"âš ï¸ Local Intelligence Core failed: {e}")
            
            # Initialize Neural Reasoning Engine
            try:
                self.neural_reasoning = NeuralReasoningEngine()
                logger.info("âœ… Neural Reasoning Engine initialized")
            except Exception as e:
                logger.warning(f"âš ï¸ Neural Reasoning Engine failed: {e}")
            
            # Initialize Personality Engine
            try:
                self.personality_engine = PersonalityEngine()
                logger.info(f"âœ… Personality Engine initialized (ID: {self.personality_engine.personality_id})")
            except Exception as e:
                logger.warning(f"âš ï¸ Personality Engine failed: {e}")
            
            # Initialize Self-Learning System
            try:
                self.self_learning = SelfLearningSystem()
                logger.info("âœ… Self-Learning System initialized")
            except Exception as e:
                logger.warning(f"âš ï¸ Self-Learning System failed: {e}")
            
            # Initialize Autonomous System (only if explicitly enabled)
            if self.enable_autonomy:
                try:
                    from companion_baas.tier4 import AutonomousSystem
                    if AutonomousSystem:
                        self.autonomous_system = AutonomousSystem()
                        logger.info("âœ… Autonomous System initialized (âš ï¸ self-modification enabled)")
                    else:
                        logger.warning("âš ï¸ Autonomous System not available")
                except Exception as e:
                    logger.warning(f"âš ï¸ Autonomous System failed: {e}")
            else:
                logger.info("ðŸ”’ Autonomous System disabled (safe mode)")
            
            # Initialize AGI Decision Engine - The autonomous intelligence core
            try:
                from companion_baas.core.agi_decision_engine import AGIDecisionEngine
                self.agi_decision_engine = AGIDecisionEngine(self)
                logger.info("ðŸ¤– AGI Decision Engine initialized - Autonomous intelligence active!")
            except Exception as e:
                logger.warning(f"âš ï¸ AGI Decision Engine failed: {e}")
            
            logger.info("ðŸŽŠ AGI Features initialized successfully!")
            
        except ImportError as e:
            logger.warning(f"âš ï¸ AGI features not available (Tier 4 modules not found): {e}")
            self.enable_agi = False
        except Exception as e:
            import traceback
            logger.error(f"âŒ AGI features initialization failed: {e}")
            logger.debug(traceback.format_exc())
            self.enable_agi = False
    
    def _initialize_thread_manager(self):
        """
        Initialize Thread Manager - Centralized thread management for ALL modules
        
        This creates a thread-based architecture where:
        - All modules run in dedicated threads
        - Brain autonomously manages thread lifecycle
        - Brain makes decisions about thread creation/destruction
        - All communication flows through thread manager
        
        Architecture:
            All modules â†’ Threads â†’ ThreadManager â†’ CompanionBrain (autonomous control)
        """
        self.thread_manager = None
        self.module_threads = {}  # Track threads for each module
        
        if not THREAD_MANAGER_AVAILABLE:
            logger.warning("âš ï¸ Thread Manager not available - modules will run synchronously")
            return
        
        try:
            logger.info("ðŸ§µ Initializing Thread Manager...")
            
            # Create thread manager with autonomous decision-making
            self.thread_manager = ThreadManager(
                max_threads=50,  # Maximum concurrent threads
                enable_auto_scaling=True  # Let brain decide when to scale
            )
            
            logger.info("âœ… Thread Manager initialized")
            
            # Create threads for all active modules
            self._create_module_threads()
            
            # Start autonomous thread management
            self._start_autonomous_thread_management()
            
            logger.info("ðŸŽŠ Thread-based architecture activated!")
            logger.info(f"ðŸ§µ Active module threads: {len(self.module_threads)}")
            
        except Exception as e:
            logger.error(f"âŒ Thread Manager initialization failed: {e}")
            self.thread_manager = None
    
    def _create_module_threads(self):
        """Create dedicated threads for all active modules"""
        if not self.thread_manager:
            return
        
        logger.info("ðŸ—ï¸ Creating module threads...")
        
        # Core threads
        self._create_core_threads()
        
        # Phase threads
        self._create_phase_threads()
        
        # Advanced feature threads
        if self.advanced:
            self._create_advanced_threads()
        
        # AGI threads
        if self.enable_agi:
            self._create_agi_threads()
        
        logger.info(f"âœ… Created {len(self.module_threads)} module threads")
    
    def _create_core_threads(self):
        """Create threads for core brain operations"""
        if not self.thread_manager:
            return
        
        # Model router thread
        def model_router_worker(thread_info, task_queue):
            """Handle model routing requests"""
            create_worker_thread(
                thread_info,
                task_queue,
                lambda task: self._route_to_model(task['message'], task.get('context', {}))
            )
        
        thread_id = self.thread_manager.create_thread(
            name="model_router",
            category="core",
            function=model_router_worker,
            priority=ThreadPriority.CRITICAL
        )
        self.module_threads['model_router'] = thread_id
        
        # Context manager thread
        def context_manager_worker(thread_info, task_queue):
            """Handle context management"""
            create_worker_thread(
                thread_info,
                task_queue,
                lambda task: self._manage_context(task['context_key'], task['operation'], task.get('data'))
            )
        
        thread_id = self.thread_manager.create_thread(
            name="context_manager",
            category="core",
            function=context_manager_worker,
            priority=ThreadPriority.HIGH
        )
        self.module_threads['context_manager'] = thread_id
        
        logger.debug("âœ… Core threads created (model_router, context_manager)")
    
    def _create_phase_threads(self):
        """Create threads for Phase 1-5 components"""
        if not self.thread_manager:
            return
        
        # Phase 1: Knowledge Layer threads
        if PHASE1_AVAILABLE and self.knowledge_retriever:
            def knowledge_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.knowledge_retriever.retrieve(task['query'])
                )
            
            thread_id = self.thread_manager.create_thread(
                name="knowledge_retriever",
                category="phase1",
                function=knowledge_worker,
                priority=ThreadPriority.HIGH
            )
            self.module_threads['knowledge_retriever'] = thread_id
            logger.debug("âœ… Phase 1 thread created (knowledge_retriever)")
        
        # Phase 2: Search Engine threads
        if PHASE2_AVAILABLE and self.search_engine:
            def search_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.search_engine.search(task['query'])
                )
            
            thread_id = self.thread_manager.create_thread(
                name="search_engine",
                category="phase2",
                function=search_worker,
                priority=ThreadPriority.MEDIUM
            )
            self.module_threads['search_engine'] = thread_id
            logger.debug("âœ… Phase 2 thread created (search_engine)")
        
        # Phase 3: Web Intelligence threads
        if PHASE3_AVAILABLE:
            if self.web_crawler:
                def crawler_worker(thread_info, task_queue):
                    create_worker_thread(
                        thread_info,
                        task_queue,
                        lambda task: self.web_crawler.crawl(task['url'])
                    )
                
                thread_id = self.thread_manager.create_thread(
                    name="web_crawler",
                    category="phase3",
                    function=crawler_worker,
                    priority=ThreadPriority.MEDIUM
                )
                self.module_threads['web_crawler'] = thread_id
            
            logger.debug("âœ… Phase 3 threads created (web_crawler)")
        
        # Phase 4: Code Execution threads
        if PHASE4_AVAILABLE and self.code_executor:
            def executor_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.code_executor.execute(task['code'], task.get('language'))
                )
            
            thread_id = self.thread_manager.create_thread(
                name="code_executor",
                category="phase4",
                function=executor_worker,
                priority=ThreadPriority.HIGH
            )
            self.module_threads['code_executor'] = thread_id
            logger.debug("âœ… Phase 4 thread created (code_executor)")
        
        # Phase 5: Optimization threads
        if PHASE5_AVAILABLE:
            def optimizer_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self._optimize_performance(task.get('metrics'))
                )
            
            thread_id = self.thread_manager.create_thread(
                name="optimizer",
                category="phase5",
                function=optimizer_worker,
                priority=ThreadPriority.LOW
            )
            self.module_threads['optimizer'] = thread_id
            logger.debug("âœ… Phase 5 thread created (optimizer)")
    
    def _create_advanced_threads(self):
        """Create threads for advanced features (8 capabilities)"""
        if not self.thread_manager or not self.advanced:
            return
        
        # Advanced reasoning thread
        def reasoning_worker(thread_info, task_queue):
            create_worker_thread(
                thread_info,
                task_queue,
                lambda task: self.advanced.reason(task['query'], task.get('depth', 3))
            )
        
        thread_id = self.thread_manager.create_thread(
            name="advanced_reasoning",
            category="advanced",
            function=reasoning_worker,
            priority=ThreadPriority.HIGH
        )
        self.module_threads['advanced_reasoning'] = thread_id
        
        # Multimodal processing thread
        def multimodal_worker(thread_info, task_queue):
            create_worker_thread(
                thread_info,
                task_queue,
                lambda task: self.advanced.process_multimodal(task['inputs'])
            )
        
        thread_id = self.thread_manager.create_thread(
            name="multimodal_processor",
            category="advanced",
            function=multimodal_worker,
            priority=ThreadPriority.MEDIUM
        )
        self.module_threads['multimodal_processor'] = thread_id
        
        logger.debug("âœ… Advanced feature threads created (reasoning, multimodal)")
    
    def _create_agi_threads(self):
        """Create threads for AGI components (Tier 4)"""
        if not self.thread_manager or not self.enable_agi:
            return
        
        # Personality engine thread
        if self.personality_engine:
            def personality_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.personality_engine.process_interaction(task['message'])
                )
            
            thread_id = self.thread_manager.create_thread(
                name="personality_engine",
                category="agi",
                function=personality_worker,
                priority=ThreadPriority.MEDIUM
            )
            self.module_threads['personality_engine'] = thread_id
        
        # Neural reasoning thread
        if self.neural_reasoning:
            def neural_reasoning_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.neural_reasoning.reason(task['query'])
                )
            
            thread_id = self.thread_manager.create_thread(
                name="neural_reasoning",
                category="agi",
                function=neural_reasoning_worker,
                priority=ThreadPriority.HIGH
            )
            self.module_threads['neural_reasoning'] = thread_id
        
        # Self-learning thread
        if self.self_learning:
            def learning_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.self_learning.learn_from_interaction(
                        task['query'],
                        task['response'],
                        task.get('feedback')
                    )
                )
            
            thread_id = self.thread_manager.create_thread(
                name="self_learning",
                category="agi",
                function=learning_worker,
                priority=ThreadPriority.MEDIUM
            )
            self.module_threads['self_learning'] = thread_id
        
        # Autonomous system thread (if enabled)
        if self.autonomous_system:
            def autonomous_worker(thread_info, task_queue):
                create_worker_thread(
                    thread_info,
                    task_queue,
                    lambda task: self.autonomous_system.make_decision(task['situation'])
                )
            
            thread_id = self.thread_manager.create_thread(
                name="autonomous_system",
                category="agi",
                function=autonomous_worker,
                priority=ThreadPriority.HIGH
            )
            self.module_threads['autonomous_system'] = thread_id
        
        logger.debug(f"âœ… AGI threads created ({len([k for k in self.module_threads.keys() if k in ['personality_engine', 'neural_reasoning', 'self_learning', 'autonomous_system']])} components)")
    
    def _start_autonomous_thread_management(self):
        """
        Start autonomous thread management system
        
        The brain will:
        - Monitor thread health
        - Make decisions about scaling
        - Optimize resource allocation
        - Self-heal failed threads
        """
        if not self.thread_manager:
            return
        
        def autonomous_manager_worker(thread_info, task_queue):
            """Autonomous decision-making for thread management"""
            import time
            
            while thread_info.state == ThreadState.RUNNING:
                try:
                    time.sleep(10)  # Check every 10 seconds
                    
                    # Get system status
                    status = self.thread_manager.get_system_status()
                    
                    # Brain makes autonomous decisions
                    decisions = self._make_thread_decisions(status)
                    
                    # Execute decisions
                    for decision in decisions:
                        self._execute_thread_decision(decision)
                    
                    thread_info.tasks_completed += 1
                    
                except Exception as e:
                    logger.error(f"âŒ Autonomous manager error: {e}")
                    thread_info.tasks_failed += 1
        
        thread_id = self.thread_manager.create_thread(
            name="autonomous_thread_manager",
            category="monitoring",
            function=autonomous_manager_worker,
            priority=ThreadPriority.HIGH,
            daemon=True
        )
        self.module_threads['autonomous_manager'] = thread_id
        
        logger.info("ðŸ¤– Autonomous thread management activated")
    
    def _make_thread_decisions(self, status: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Brain makes autonomous decisions about thread management
        
        Args:
            status: Current system status
            
        Returns:
            List of decisions to execute
        """
        decisions = []
        
        # Decision 1: Scale up if high load
        for category, cat_status in status['categories'].items():
            if cat_status['active'] == cat_status['max']:
                # All threads busy - scale up
                decisions.append({
                    'action': 'scale_up',
                    'category': category,
                    'reason': 'All threads busy'
                })
        
        # Decision 2: Scale down if idle
        for category, cat_status in status['categories'].items():
            if cat_status['active'] < cat_status['total'] * 0.3:
                # Many idle threads - scale down
                decisions.append({
                    'action': 'scale_down',
                    'category': category,
                    'reason': 'Low utilization'
                })
        
        # Decision 3: Restart failed threads
        for thread_id, thread_info in self.thread_manager.threads.items():
            if thread_info.state == ThreadState.ERROR:
                decisions.append({
                    'action': 'restart',
                    'thread_id': thread_id,
                    'reason': 'Thread error detected'
                })
        
        return decisions
    
    def _execute_thread_decision(self, decision: Dict[str, Any]):
        """Execute a thread management decision"""
        action = decision['action']
        
        try:
            if action == 'scale_up':
                logger.info(f"ðŸ”¼ Brain decision: Scale up {decision['category']} ({decision['reason']})")
                # Auto-scaling handled by ThreadManager
            
            elif action == 'scale_down':
                logger.info(f"ðŸ”½ Brain decision: Scale down {decision['category']} ({decision['reason']})")
                # Could implement scale-down logic here
            
            elif action == 'restart':
                logger.info(f"ðŸ”„ Brain decision: Restart thread {decision['thread_id']} ({decision['reason']})")
                self.thread_manager.restart_thread(decision['thread_id'])
        
        except Exception as e:
            logger.error(f"âŒ Failed to execute decision {action}: {e}")
    
    def _route_to_model(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Route request to appropriate model (used by model_router thread)"""
        # Implementation of model routing logic
        return {'routed': True, 'model': 'default'}
    
    def _manage_context(self, context_key: str, operation: str, data: Any) -> Dict[str, Any]:
        """Manage context operations (used by context_manager thread)"""
        # Implementation of context management logic
        return {'success': True, 'operation': operation}
    
    def _optimize_performance(self, metrics: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Optimize performance based on metrics (used by optimizer thread)"""
        # Implementation of optimization logic
        return {'optimized': True}
    
    def _get_enabled_phases(self) -> List[str]:
        """Get list of enabled phases"""
        phases = []
        if PHASE1_AVAILABLE and self.knowledge_retriever:
            phases.append("Phase 1: Knowledge")
        if PHASE2_AVAILABLE and self.search_engine:
            phases.append("Phase 2: Search")
        if PHASE3_AVAILABLE and self.web_crawler:
            phases.append("Phase 3: Web Intelligence")
        if PHASE4_AVAILABLE and self.code_executor:
            phases.append("Phase 4: Execution")
        if PHASE5_AVAILABLE and self.profiler:
            phases.append("Phase 5: Optimization")
        if ADVANCED_FEATURES_AVAILABLE and hasattr(self, 'advanced') and self.advanced:
            phases.append("Advanced Features (8 capabilities)")
        if self.enable_agi:
            agi_components = []
            if self.local_intelligence: agi_components.append("Local Intelligence")
            if self.neural_reasoning: agi_components.append("Neural Reasoning")
            if self.personality_engine: agi_components.append("Personality")
            if self.self_learning: agi_components.append("Self-Learning")
            if self.autonomous_system: agi_components.append("Autonomous")
            if agi_components:
                phases.append(f"Tier 4 AGI ({', '.join(agi_components)})")
        if not phases:
            phases.append("Legacy components only")
        return phases
    
    def _register_default_prompt_variants(self):
        """Register default prompt variants for A/B testing"""
        # Code generation task variants
        self.prompt_optimizer.register_variant(
            'code_generation',
            'detailed',
            'Generate clean, well-documented code for the following task:\n{task}\n\nRequirements: Include comments, error handling, and follow best practices.'
        )
        self.prompt_optimizer.register_variant(
            'code_generation',
            'concise',
            'Write efficient code for: {task}\n\nFocus on brevity and performance.'
        )
        
        # Explanation task variants
        self.prompt_optimizer.register_variant(
            'explanation',
            'detailed',
            'Provide a comprehensive explanation of:\n{topic}\n\nInclude examples, analogies, and step-by-step breakdown.'
        )
        self.prompt_optimizer.register_variant(
            'explanation',
            'simple',
            'Explain {topic} in simple terms that anyone can understand.'
        )
        
        # Debugging task variants
        self.prompt_optimizer.register_variant(
            'debugging',
            'systematic',
            'Analyze this code for bugs:\n{code}\n\nProvide: 1) Issue identification, 2) Root cause, 3) Fix, 4) Prevention tips.'
        )
        self.prompt_optimizer.register_variant(
            'debugging',
            'quick_fix',
            'Find and fix bugs in:\n{code}\n\nProvide corrected code with brief explanation.'
        )
    
    # ============================================================================
    # NEW PHASE METHODS
    # ============================================================================
    
    def execute_code(
        self,
        code: str,
        language: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Execute code using Phase 4 code executor
        
        Args:
            code: Code to execute
            language: Programming language ('python', 'javascript', or auto-detect)
            context: Execution context
            
        Returns:
            Execution result with output and metadata
        """
        if not self.code_executor:
            return {
                'success': False,
                'error': 'Phase 4 (Code Execution) not available',
                'output': None
            }
        
        try:
            if self.profiler:
                with self.profiler.measure("code_execution"):
                    result = self.code_executor.execute(code, language)
            else:
                result = self.code_executor.execute(code, language)
            
            if self.monitor:
                self.monitor.record_request(
                    "code_execution",
                    result.execution_time,
                    "success" if result.success else "failed"
                )
            
            return {
                'success': result.success,
                'output': result.output,
                'error': result.error,
                'language': result.language,
                'execution_time': result.execution_time
            }
        except Exception as e:
            logger.error(f"Code execution error: {e}")
            return {
                'success': False,
                'error': str(e),
                'output': None
            }
    
    def call_tool(
        self,
        tool_name: str,
        *args,
        use_cache: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Call a registered tool from Phase 4
        
        Args:
            tool_name: Name of the tool to call
            *args: Positional arguments
            use_cache: Whether to use caching
            **kwargs: Keyword arguments
            
        Returns:
            Tool execution result
        """
        if not self.tool_executor:
            return {
                'success': False,
                'error': 'Phase 4 (Tool Executor) not available',
                'result': None
            }
        
        try:
            if self.profiler:
                with self.profiler.measure(f"tool_{tool_name}"):
                    result = self.tool_executor.execute(tool_name, *args, use_cache=use_cache, **kwargs)
            else:
                result = self.tool_executor.execute(tool_name, *args, use_cache=use_cache, **kwargs)
            
            if self.monitor:
                self.monitor.record_request(f"tool_{tool_name}", 0.001, "success")
            
            return {
                'success': True,
                'result': result.result,
                'cached': result.cached,
                'execution_time': result.execution_time
            }
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            if self.monitor:
                self.monitor.record_error("tool_error", str(e))
            return {
                'success': False,
                'error': str(e),
                'result': None
            }
    
    def list_available_tools(self) -> List[str]:
        """List all available tools from Phase 4"""
        if not self.tool_registry:
            return []
        return self.tool_registry.list_tools()
    
    def semantic_search(
        self,
        query: str,
        top_k: int = 5,
        user_id: Optional[str] = None,
        app_type: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Perform semantic search using Phase 1 knowledge layer
        
        Args:
            query: Search query
            top_k: Number of results to return
            user_id: Filter by user ID
            app_type: Filter by app type
            
        Returns:
            Search results with similarity scores
        """
        if not self.knowledge_retriever:
            return {
                'success': False,
                'error': 'Phase 1 (Knowledge Layer) not available',
                'results': []
            }
        
        try:
            results = self.knowledge_retriever.search(query, top_k=top_k, user_id=user_id, app_type=app_type)
            return {
                'success': True,
                'results': results
            }
        except Exception as e:
            logger.error(f"Semantic search error: {e}")
            return {
                'success': False,
                'error': str(e),
                'results': []
            }
    
    def hybrid_search(
        self,
        query: str,
        index_name: str = "knowledge",
        limit: int = 10
    ) -> Dict[str, Any]:
        """
        Perform hybrid search (text + semantic) using Phase 2
        
        Args:
            query: Search query
            index_name: Index to search
            limit: Number of results
            
        Returns:
            Hybrid search results
        """
        if not self.search_engine:
            return {
                'success': False,
                'error': 'Phase 2 (Search Engine) not available',
                'results': []
            }
        
        try:
            results = self.search_engine.hybrid_search(query, index_name=index_name, limit=limit)
            return {
                'success': True,
                'results': results.get('hits', [])
            }
        except Exception as e:
            logger.error(f"Hybrid search error: {e}")
            return {
                'success': False,
                'error': str(e),
                'results': []
            }
    
    def crawl_web(
        self,
        url: str,
        extract_content: bool = True
    ) -> Dict[str, Any]:
        """
        Crawl a webpage using Phase 3 web intelligence
        
        Args:
            url: URL to crawl
            extract_content: Whether to extract main content
            
        Returns:
            Crawled content and metadata
        """
        if not self.web_crawler:
            return {
                'success': False,
                'error': 'Phase 3 (Web Intelligence) not available',
                'content': None
            }
        
        try:
            result = self.web_crawler.crawl(url)
            return {
                'success': result.get('success', False),
                'content': result.get('content'),
                'metadata': result.get('metadata', {})
            }
        except Exception as e:
            logger.error(f"Web crawling error: {e}")
            return {
                'success': False,
                'error': str(e),
                'content': None
            }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get performance statistics from Phase 5 monitoring"""
        stats = self.get_stats()
        
        if self.monitor:
            stats['monitoring'] = {
                'cache_hit_rate': self.monitor.get_cache_hit_rate('brain_cache'),
                'memory_mb': self.profiler.get_memory_usage()['rss_mb'] if self.profiler else None,
                'cpu_percent': self.profiler.get_cpu_usage()['percent'] if self.profiler else None
            }
        
        if self.cache_optimizer:
            stats['cache_stats'] = self.cache_optimizer.get_stats()
        
        return stats
    
    # ============================================================================
    # ADVANCED FEATURES METHODS (8 Capabilities)
    # ============================================================================
    
    def _call_llm(self, prompt: str, **kwargs) -> str:
        """
        Internal LLM caller for advanced features
        Uses existing brain's API infrastructure
        """
        try:
            # For autonomous mode, prefer OpenRouter (more reliable than Bytez)
            if self.app_type == 'autonomous' and hasattr(self, 'openrouter_api'):
                try:
                    result = self.think(
                        message=prompt,
                        model='google/gemini-2.0-flash-exp:free',
                        use_agi_decision=False
                    )
                    if result.get('success'):
                        return result.get('response', 'No response')
                except Exception as e:
                    logger.warning(f"OpenRouter call failed: {e}, trying Bytez...")
            
            # Use legacy API wrapper if available
            if LEGACY_AVAILABLE and api_wrapper:
                response = generate_companion_response(prompt, history=[])
                if isinstance(response, APIResponse):
                    return response.content
                return str(response)
            
            # Fallback: Use Bytez if available (but it's slow)
            if BYTEZ_AVAILABLE and 'bytez' in self.providers:
                bytez = self.providers['bytez']
                model = self._route_to_best_model(prompt)

                def _call():
                    # prefer chat if available, else generate
                    if hasattr(bytez, 'chat'):
                        return bytez.chat(prompt, model=model)
                    return bytez.generate(messages=[{'role': 'user', 'content': prompt}], model=model)

                result = self._call_with_retry(_call, provider='bytez', model=model)
                return result
            
            return "LLM not available"
        except Exception as e:
            logger.error(f"LLM call error: {e}")
            return f"Error: {e}"
    
    async def think_advanced(
        self,
        query: str,
        user_id: str = "default",
        use_reasoning: bool = False,
        use_memory: bool = True,
        use_agents: bool = False,
        stream: bool = False,
        media_inputs: Optional[List] = None
    ):
        """
        Advanced thinking with all 8 capabilities
        
        Args:
            query: User query
            user_id: User identifier
            use_reasoning: Enable advanced reasoning (CoT, ToT, etc.)
            use_memory: Enable memory systems
            use_agents: Enable multi-agent coordination
            stream: Enable streaming responses
            media_inputs: Optional media inputs (images, audio, video, documents)
            
        Returns:
            Response (string or async stream)
        """
        if not self.advanced:
            return "Advanced features not available"
        
        # Convert media inputs to MediaInput objects if needed
        if media_inputs and ADVANCED_FEATURES_AVAILABLE:
            media_objs = []
            for item in media_inputs:
                if isinstance(item, dict):
                    media_objs.append(MediaInput(
                        type=MediaType(item.get('type', 'text')),
                        content=item.get('content'),
                        metadata=item.get('metadata', {})
                    ))
                else:
                    media_objs.append(item)
            media_inputs = media_objs
        
        return await self.advanced.think(
            query=query,
            user_id=user_id,
            use_reasoning=use_reasoning,
            use_memory=use_memory,
            use_agents=use_agents,
            stream=stream,
            media_inputs=media_inputs
        )
    
    def reason(self, query: str, strategy: str = "auto", user_id: str = "default"):
        """
        Advanced reasoning (Chain-of-Thought, Tree-of-Thought, etc.)
        
        Args:
            query: Query to reason about
            strategy: 'auto', 'chain_of_thought', 'tree_of_thought', 'self_reflection', 'react'
            user_id: User identifier
            
        Returns:
            Reasoning result with steps and answer
        """
        if not self.advanced:
            return {"error": "Advanced features not available"}
        
        return self.advanced.reason(query, strategy, user_id)
    
    def process_media(
        self,
        media_inputs: List,
        prompt: str,
        user_id: str = "default"
    ):
        """
        Process multi-modal inputs (images, audio, video, documents)
        
        Args:
            media_inputs: List of MediaInput objects or dicts
            prompt: Processing prompt
            user_id: User identifier
            
        Returns:
            List of processing results
        """
        if not self.advanced:
            return [{"error": "Advanced features not available"}]
        
        # Convert to MediaInput objects
        if ADVANCED_FEATURES_AVAILABLE:
            media_objs = []
            for item in media_inputs:
                if isinstance(item, dict):
                    media_objs.append(MediaInput(
                        type=MediaType(item.get('type', 'text')),
                        content=item.get('content'),
                        metadata=item.get('metadata', {})
                    ))
                else:
                    media_objs.append(item)
            media_inputs = media_objs
        
        return self.advanced.process_media(media_inputs, prompt, user_id)
    
    async def stream_think(
        self,
        query: str,
        user_id: str = "default",
        show_reasoning: bool = True
    ):
        """
        Stream thinking process with real-time updates
        
        Args:
            query: Query to process
            user_id: User identifier
            show_reasoning: Show reasoning steps
            
        Yields:
            Stream chunks with events, content, metadata
        """
        if not self.advanced:
            yield {"error": "Advanced features not available"}
            return
        
        async for chunk in self.advanced.stream_think(query, user_id, show_reasoning):
            yield chunk
    
    async def delegate_task(
        self,
        task: str,
        use_multiple_agents: bool = True,
        decompose: bool = True
    ) -> str:
        """
        Delegate task to specialized agents
        
        Args:
            task: Task description
            use_multiple_agents: Use multiple agents for consensus
            decompose: Break into subtasks
            
        Returns:
            Task result
        """
        if not self.advanced:
            return "Advanced features not available"
        
        return await self.advanced.delegate_task(task, use_multiple_agents, decompose)
    
    def provide_feedback(
        self,
        user_id: str,
        interaction_id: str,
        feedback_type: str,
        value: Any
    ):
        """
        Provide feedback for real-time learning
        
        Args:
            user_id: User identifier
            interaction_id: Interaction ID
            feedback_type: 'positive', 'negative', 'rating', 'correction', 'preference'
            value: Feedback value
        """
        if not self.advanced:
            return
        
        self.advanced.provide_feedback(user_id, interaction_id, feedback_type, value)
    
    def remember(
        self,
        user_id: str,
        content: str,
        memory_type: str = "long_term",
        importance: float = 0.7
    ):
        """
        Store memory for user
        
        Args:
            user_id: User identifier
            content: Content to remember
            memory_type: 'short_term', 'long_term', 'semantic', 'episodic', 'procedural', 'preference'
            importance: Importance score (0.0-1.0)
        """
        if not self.advanced:
            return
        
        self.advanced.remember(user_id, content, memory_type, importance)
    
    def recall(self, user_id: str, query: str, limit: int = 5):
        """
        Recall memories for user
        
        Args:
            user_id: User identifier
            query: Search query
            limit: Maximum results
            
        Returns:
            List of memories
        """
        if not self.advanced:
            return []
        
        return self.advanced.recall(user_id, query, limit)
    
    def get_advanced_capabilities(self):
        """Get status of all advanced capabilities"""
        if not self.advanced:
            return {"enabled": False}
        
        return {
            "enabled": True,
            "capabilities": self.advanced.get_capabilities(),
            "usage_stats": self.advanced.get_usage_stats(),
            "system_status": self.advanced.get_system_status()
        }
    
    # ============================================================================
    # CIRCUIT BREAKER MANAGEMENT (Tier 2)
    # ============================================================================
    
    def get_circuit_breaker_status(self) -> Dict[str, Any]:
        """Get status of all circuit breakers"""
        return {
            name: breaker.get_state() 
            for name, breaker in self.circuit_breakers.items()
        }
    
    def reset_circuit_breaker(self, component: str) -> bool:
        """
        Manually reset a circuit breaker for a specific component
        
        Args:
            component: Component name ('bytez', 'elasticsearch', etc.)
            
        Returns:
            True if reset successful, False if component not found
        """
        if component in self.circuit_breakers:
            self.circuit_breakers[component].reset()
            logger.info(f"ðŸ”„ Manually reset circuit breaker for '{component}'")
            return True
        else:
            logger.warning(f"âš ï¸ Circuit breaker '{component}' not found")
            return False
    
    def reset_all_circuit_breakers(self):
        """Reset all circuit breakers"""
        for name, breaker in self.circuit_breakers.items():
            breaker.reset()
        logger.info(f"ðŸ”„ Reset all {len(self.circuit_breakers)} circuit breakers")
    
    # ============================================================================
    # ASYNC THINKING (Tier 2 - Parallel Phase Execution)
    # ============================================================================
    
    async def think_async(
        self,
        message: str,
        context: Optional[Dict[str, Any]] = None,
        tools: Optional[List[str]] = None,
        user_id: Optional[str] = None,
        conversation_id: Optional[str] = None,
        parallel_phases: bool = True
    ) -> Dict[str, Any]:
        """
        Async version of think() that can run independent phases in parallel.
        Provides 3-5x speedup for complex multi-phase queries.
        
        Args:
            message: User's input message/query
            context: Application-specific context
            tools: List of tools to use
            user_id: User identifier
            conversation_id: Conversation ID
            parallel_phases: If True, run independent phases concurrently
            
        Returns:
            Dict with response, metadata, and success status
        """
        self.request_count += 1
        self.stats['total_requests'] += 1
        start_time = datetime.now()
        
        try:
            # Setup context (same as sync version)
            context_key = conversation_id or user_id or "default"
            if context_key not in self.contexts:
                self.contexts[context_key] = {
                    'history': [],
                    'metadata': context or {},
                    'created_at': datetime.now()
                }
            
            conversation_context = self.contexts[context_key]
            
            if context:
                conversation_context['metadata'].update(context)
            
            if tools is None:
                tools = self._get_default_tools_for_app_type()
            
            # Add message to history
            conversation_context['history'].append({
                'role': 'user',
                'content': message,
                'timestamp': datetime.now().isoformat()
            })
            
            # Manage context window
            try:
                self._manage_context_window(conversation_context)
            except Exception as e:
                logger.debug(f"Context window management failed: {e}")
            
            # Parallel phase execution
            if parallel_phases:
                logger.info(f"ðŸš€ Running phases in parallel for faster response")
                results = await self._execute_phases_parallel(message, tools, conversation_context)
            else:
                # Sequential fallback
                results = await self._execute_phases_sequential(message, tools, conversation_context)
            
            # Generate final response with gathered context
            response_text = await self._generate_final_response(message, results, conversation_context)
            
            # Add response to history
            conversation_context['history'].append({
                'role': 'assistant',
                'content': response_text,
                'timestamp': datetime.now().isoformat()
            })
            
            response_time = (datetime.now() - start_time).total_seconds()
            self.success_count += 1
            self.stats['successful_requests'] += 1
            
            logger.info(f"âœ… Async response generated in {response_time:.2f}s")
            
            return {
                'response': response_text,
                'metadata': {
                    'session_id': self.session_id,
                    'app_type': self.app_type,
                    'response_time': response_time,
                    'parallel_execution': parallel_phases,
                    'phases_used': list(results.keys())
                },
                'success': True
            }
            
        except Exception as e:
            logger.error(f"âŒ Async think error: {e}")
            self.stats['failed_requests'] += 1
            return {
                'response': f"I encountered an error: {str(e)}",
                'metadata': {
                    'session_id': self.session_id,
                    'error': str(e)
                },
                'success': False,
                'error': str(e)
            }
    
    async def _execute_phases_parallel(self, message: str, tools: List[str], context: Dict) -> Dict[str, Any]:
        """Execute independent phases concurrently"""
        tasks = []
        phase_names = []
        
        # Phase 1: Knowledge retrieval (if available)
        if self.knowledge_retriever and 'research' in tools:
            tasks.append(self._async_knowledge_lookup(message))
            phase_names.append('knowledge')
        
        # Phase 3: Web search (if requested)
        if 'web' in tools or 'deepsearch' in tools:
            tasks.append(self._async_web_search(message, deep='deepsearch' in tools))
            phase_names.append('web')
        
        # Phase 4: Code analysis (if code-related)
        if 'code' in tools or any(kw in message.lower() for kw in ['code', 'function', 'script']):
            tasks.append(self._async_code_analysis(message))
            phase_names.append('code')
        
        # Execute all tasks concurrently
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return {name: result for name, result in zip(phase_names, results) if not isinstance(result, Exception)}
        
        return {}
    
    async def _execute_phases_sequential(self, message: str, tools: List[str], context: Dict) -> Dict[str, Any]:
        """Execute phases sequentially (fallback)"""
        results = {}
        
        if self.knowledge_retriever and 'research' in tools:
            results['knowledge'] = await self._async_knowledge_lookup(message)
        
        if 'web' in tools or 'deepsearch' in tools:
            results['web'] = await self._async_web_search(message, deep='deepsearch' in tools)
        
        if 'code' in tools:
            results['code'] = await self._async_code_analysis(message)
        
        return results
    
    async def _async_knowledge_lookup(self, query: str) -> Dict[str, Any]:
        """Async knowledge retrieval"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.knowledge_retriever.search(query) if self.knowledge_retriever else None)
            return {'success': True, 'data': result}
        except Exception as e:
            logger.warning(f"Knowledge lookup failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _async_web_search(self, query: str, deep: bool = False) -> Dict[str, Any]:
        """Async web search"""
        try:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, lambda: self.search_web(query, deep_search=deep))
            return result
        except Exception as e:
            logger.warning(f"Web search failed: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _async_code_analysis(self, message: str) -> Dict[str, Any]:
        """Async code analysis"""
        try:
            # Extract code if present
            if '```' in message:
                return {'success': True, 'has_code': True}
            return {'success': True, 'has_code': False}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _generate_final_response(self, message: str, phase_results: Dict[str, Any], context: Dict) -> str:
        """Generate final response using phase results and LLM"""
        try:
            # Build enriched prompt with phase results
            enrichment = []
            
            if 'knowledge' in phase_results and phase_results['knowledge'].get('success'):
                enrichment.append(f"Knowledge context: {phase_results['knowledge'].get('data', 'N/A')}")
            
            if 'web' in phase_results and phase_results['web'].get('success'):
                enrichment.append(f"Web search results available")
            
            if 'code' in phase_results and phase_results['code'].get('has_code'):
                enrichment.append(f"Code analysis completed")
            
            # Build final prompt
            if enrichment:
                enriched_message = f"{message}\n\nContext:\n" + "\n".join(enrichment)
            else:
                enriched_message = message
            
            # Generate response using LLM
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(None, lambda: self._call_llm(enriched_message))
            
            return response
        except Exception as e:
            logger.error(f"Final response generation failed: {e}")
            return f"I processed your request but encountered an error generating the response: {e}"
    
    # ============================================================================
    # AGI FEATURES (Tier 4) - Enhanced Intelligence Methods
    # ============================================================================
    
    def get_personality(self) -> Optional[Dict[str, Any]]:
        """Get current personality state (AGI feature)"""
        if not self.enable_agi or not self.personality_engine:
            return None
        
        # Get dominant traits (top 3)
        traits_dict = self.personality_engine.traits.to_dict()
        sorted_traits = sorted(traits_dict.items(), key=lambda x: x[1], reverse=True)
        dominant_traits = [name for name, _ in sorted_traits[:3]]
        
        return {
            'personality_id': self.personality_engine.personality_id,
            'traits': traits_dict,
            'emotion': self.personality_engine.emotional_state.current_emotion.value,
            'emotion_intensity': self.personality_engine.emotional_state.emotion_intensity,
            'dominant_traits': dominant_traits
        }
    
    def get_learning_stats(self) -> Optional[Dict[str, Any]]:
        """Get learning system statistics (AGI feature)"""
        if not self.enable_agi or not self.self_learning:
            return None
        
        # Calculate average success rate from strategies
        success_rate = 0.0
        if self.self_learning.meta_learner.strategies:
            total_success = sum(s.get_success_rate() 
                              for s in self.self_learning.meta_learner.strategies.values())
            success_rate = total_success / len(self.self_learning.meta_learner.strategies)
        
        return {
            'episodes': len(self.self_learning.episodic.episodes),
            'concepts': len(self.self_learning.semantic.concepts),
            'skills': len(self.self_learning.procedural.skills),
            'mastered_skills': len([s for s in self.self_learning.procedural.skills.values() if s['proficiency'] >= 0.9]),
            'strategies': len(self.self_learning.meta_learner.strategies),
            'success_rate': success_rate
        }
    
    def get_autonomous_stats(self) -> Dict[str, Any]:
        """Get autonomous system statistics (AGI feature)"""
        if not self.enable_autonomy or not self.autonomous_system:
            return {
                'enabled': False,
                'decisions_made': 0,
                'modifications_applied': 0,
                'tasks_completed': 0,
                'improvement_cycles': 0
            }
        
        return {
            'enabled': True,
            'decisions_made': len(self.autonomous_system.decision_engine.decision_history),
            'modifications_applied': len(self.autonomous_system.self_modification.modification_history),
            'tasks_completed': len([t for t in self.autonomous_system.task_executor.tasks.values() if t['status'] == 'completed']),
            'improvement_cycles': self.autonomous_system.improvement_loop.improvement_count
        }
    
    def teach_concept(self, concept_name: str, examples: List[str]) -> bool:
        """Teach a new concept to the brain (AGI feature)"""
        if not self.enable_agi or not self.neural_reasoning:
            logger.warning("AGI features not enabled - cannot teach concepts")
            return False
        
        try:
            for example in examples:
                self.neural_reasoning.form_concept(concept_name, example)
            logger.info(f"âœ… Taught concept '{concept_name}' with {len(examples)} examples")
            return True
        except Exception as e:
            logger.error(f"Failed to teach concept: {e}")
            return False
    
    def think_with_agi(self, query: str, mode: str = "auto") -> Dict[str, Any]:
        """
        Enhanced thinking with AGI features
        
        Args:
            query: The question or task
            mode: Thinking mode (auto, reasoning, creative, conceptual)
        
        Returns:
            Dict with response and AGI metadata
        """
        if not self.enable_agi:
            # Fall back to regular thinking
            return self.think(query)
        
        try:
            start_time = time.time()
            
            # Step 1: Neural reasoning (if enabled)
            reasoning_result = None
            if self.neural_reasoning:
                if mode == "reasoning" or mode == "auto":
                    reasoning_result = self.neural_reasoning.chain_of_thought(query, steps=5)
                elif mode == "creative":
                    # Use creative synthesis
                    reasoning_result = {"thoughts": [query], "conclusion": query}
                elif mode == "conceptual":
                    # Form concepts
                    self.neural_reasoning.form_concept("query_concept", query)
                    reasoning_result = {"thoughts": [query], "conclusion": query}
            
            # Step 2: Generate base response
            base_response = self.think(query)
            
            # Step 3: Apply personality styling (if enabled)
            if self.personality_engine and base_response.get('success'):
                styled_response = self.personality_engine.style_response(
                    base_response['response'],
                    query
                )
                base_response['response'] = styled_response
                
                # Update emotion based on interaction
                self.personality_engine.update_emotion("analytical" if "?" in query else "neutral")
            
            # Step 4: Learn from interaction (if enabled)
            if self.self_learning:
                self.self_learning.episodic.store_episode(
                    query,
                    base_response['response'],
                    {"mode": mode, "reasoning": reasoning_result},
                    {},  # outcome
                    "neutral"  # emotions
                )
            
            # Add AGI metadata
            base_response['agi_metadata'] = {
                'mode': mode,
                'reasoning_used': reasoning_result is not None,
                'personality_applied': self.personality_engine is not None,
                'learning_recorded': self.self_learning is not None,
                'processing_time': time.time() - start_time
            }
            
            return base_response
            
        except Exception as e:
            logger.error(f"AGI thinking failed: {e}")
            # Graceful fallback to regular thinking
            return self.think(query)
    
    def enable_agi_mode(self, enable: bool = True):
        """Toggle AGI features on/off at runtime"""
        self.enable_agi = enable
        if enable and not any([self.local_intelligence, self.neural_reasoning, 
                               self.personality_engine, self.self_learning]):
            # Try to initialize if components not loaded
            self._initialize_agi_features()
        logger.info(f"{'âœ… AGI mode enabled' if enable else 'âš ï¸ AGI mode disabled'}")
    
    def enable_autonomy_mode(self, enable: bool = True, auto_approve_low_risk: bool = False):
        """Toggle autonomous mode on/off at runtime"""
        if enable and not self.autonomous_system:
            try:
                from companion_baas.tier4 import AutonomousSystem
                if AutonomousSystem:
                    self.autonomous_system = AutonomousSystem()
                    logger.info("âœ… Autonomous System initialized")
                else:
                    logger.error("Autonomous System not available")
                    return False
            except Exception as e:
                logger.error(f"Failed to initialize autonomous system: {e}")
                return False
        
        self.enable_autonomy = enable
        logger.info(f"{'ðŸ¤– Autonomy enabled (self-modification allowed)' if enable else 'ðŸ”’ Autonomy disabled (safe mode)'}")
        return True
    
    def get_agi_status(self) -> Dict[str, Any]:
        """Get comprehensive AGI system status"""
        return {
            'agi_enabled': self.enable_agi,
            'autonomy_enabled': self.enable_autonomy,
            'components': {
                'local_intelligence': self.local_intelligence is not None,
                'neural_reasoning': self.neural_reasoning is not None,
                'personality': self.personality_engine is not None,
                'self_learning': self.self_learning is not None,
                'autonomous': self.autonomous_system is not None
            },
            'personality': self.get_personality(),
            'learning_stats': self.get_learning_stats(),
            'autonomous_stats': self.get_autonomous_stats()
        }
    
    # ============================================================================
    # THREAD MANAGEMENT METHODS
    # ============================================================================
    
    def get_thread_status(self) -> Dict[str, Any]:
        """
        Get comprehensive thread management status
        
        Returns thread manager status including:
        - Total threads and active threads
        - Status by category (core, phases, advanced, AGI)
        - Thread health and performance metrics
        - Autonomous decisions made
        
        Example:
            brain = CompanionBrain(app_type="chatbot", enable_agi=True)
            status = brain.get_thread_status()
            print(f"Active threads: {status['active_threads']}/{status['total_threads']}")
        """
        if not self.thread_manager:
            return {
                'enabled': False,
                'message': 'Thread manager not initialized'
            }
        
        system_status = self.thread_manager.get_system_status()
        
        # Get individual thread details for module threads
        module_threads_status = {}
        for module_name, thread_id in self.module_threads.items():
            thread_status = self.thread_manager.get_thread_status(thread_id)
            if thread_status:
                module_threads_status[module_name] = {
                    'state': thread_status['state'],
                    'tasks_completed': thread_status['tasks_completed'],
                    'success_rate': thread_status['success_rate'],
                    'uptime': thread_status['uptime'],
                    'queue_size': thread_status['queue_size']
                }
        
        return {
            'enabled': True,
            'total_threads': system_status['total_threads'],
            'active_threads': system_status['active_threads'],
            'max_threads': system_status['max_threads'],
            'auto_scaling': system_status['auto_scaling'],
            'uptime': system_status['uptime'],
            'categories': system_status['categories'],
            'module_threads': module_threads_status,
            'stats': system_status['stats']
        }
    
    def get_module_thread_status(self, module_name: str) -> Optional[Dict[str, Any]]:
        """
        Get status of a specific module's thread
        
        Args:
            module_name: Module name (e.g., 'model_router', 'personality_engine')
            
        Returns:
            Thread status or None if not found
            
        Example:
            status = brain.get_module_thread_status('personality_engine')
            print(f"Personality engine thread: {status['state']}")
        """
        if not self.thread_manager or module_name not in self.module_threads:
            return None
        
        thread_id = self.module_threads[module_name]
        return self.thread_manager.get_thread_status(thread_id)
    
    def submit_task_to_module(
        self,
        module_name: str,
        function: Callable,
        args: tuple = (),
        kwargs: Optional[Dict[str, Any]] = None,
        priority: str = 'medium'
    ) -> Optional[str]:
        """
        Submit a task to a specific module's thread
        
        Args:
            module_name: Module name
            function: Function to execute
            args: Function arguments
            kwargs: Function keyword arguments
            priority: Task priority ('critical', 'high', 'medium', 'low', 'idle')
            
        Returns:
            Task ID or None if failed
            
        Example:
            # Submit personality analysis task
            task_id = brain.submit_task_to_module(
                'personality_engine',
                lambda msg: brain.personality_engine.analyze(msg),
                args=("Hello!",),
                priority='high'
            )
        """
        if not self.thread_manager or module_name not in self.module_threads:
            return None
        
        # Convert priority string to ThreadPriority enum
        priority_map = {
            'critical': ThreadPriority.CRITICAL,
            'high': ThreadPriority.HIGH,
            'medium': ThreadPriority.MEDIUM,
            'low': ThreadPriority.LOW,
            'idle': ThreadPriority.IDLE
        }
        thread_priority = priority_map.get(priority.lower(), ThreadPriority.MEDIUM)
        
        thread_id = self.module_threads[module_name]
        return self.thread_manager.submit_task(
            thread_id,
            function=function,
            args=args,
            kwargs=kwargs or {},
            priority=thread_priority
        )
    
    def get_thread_decisions_history(self) -> List[Dict[str, Any]]:
        """
        Get history of autonomous thread management decisions
        
        Returns list of decisions made by the brain including:
        - Scale up/down decisions
        - Thread restart decisions
        - Resource optimization decisions
        
        Example:
            decisions = brain.get_thread_decisions_history()
            for decision in decisions[-10:]:  # Last 10 decisions
                print(f"{decision['type']}: {decision['context']} - {decision['reason']}")
        """
        if not self.thread_manager:
            return []
        
        return self.thread_manager.decision_system.decision_history
    
    def pause_module_thread(self, module_name: str) -> bool:
        """
        Pause a specific module's thread
        
        Args:
            module_name: Module name
            
        Returns:
            True if successful
            
        Example:
            brain.pause_module_thread('web_crawler')  # Temporarily pause web crawling
        """
        if not self.thread_manager or module_name not in self.module_threads:
            return False
        
        thread_id = self.module_threads[module_name]
        self.thread_manager.pause_thread(thread_id)
        return True
    
    def resume_module_thread(self, module_name: str) -> bool:
        """
        Resume a paused module's thread
        
        Args:
            module_name: Module name
            
        Returns:
            True if successful
            
        Example:
            brain.resume_module_thread('web_crawler')  # Resume web crawling
        """
        if not self.thread_manager or module_name not in self.module_threads:
            return False
        
        thread_id = self.module_threads[module_name]
        self.thread_manager.resume_thread(thread_id)
        return True
    
    def shutdown_threads(self, timeout: float = 10.0):
        """
        Gracefully shutdown all threads
        
        Args:
            timeout: Maximum time to wait for shutdown
            
        Example:
            brain.shutdown_threads(timeout=15.0)
        """
        if self.thread_manager:
            logger.info("ðŸ›‘ Shutting down thread manager...")
            self.thread_manager.shutdown(timeout=timeout)
        
    def __repr__(self):
        agi_status = " [AGI]" if self.enable_agi else ""
        autonomous_status = " [AUTONOMOUS]" if self.enable_autonomy else ""
        return f"<CompanionBrain app_type='{self.app_type}' session={self.session_id[:8]} requests={self.request_count}{agi_status}{autonomous_status}>"
